{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Python/3.7/site-packages/IPython/core/interactiveshell.py:3063: DtypeWarning: Columns (21) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from os import path, getcwd\n",
    "\n",
    "PROCESSED_DATA_PATH = \"../data/hydrated_data_ieee/final_data/processed.csv\"\n",
    "\n",
    "df = pd.read_csv(PROCESSED_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/tingfengx/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/tingfengx/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/tingfengx/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        # for safety\n",
    "        return wordnet.NOUN\n",
    "    \n",
    "sad_emoticons = {\":-(\", \":(\", \":-|\", \";-(\", \";-<\", \"|-{\"}\n",
    "happy_emoticons = {\":-)\", \":)\", \":o)\", \":-}\", \";-}\", \":->\", \";-)\", \":p\", \":D\", \";p\", \"XD\"}\n",
    "\n",
    "dct = {\n",
    "    'gr8': 'great', \n",
    "    'awsm': 'awesome',\n",
    "    'omfg': 'oh my fucking god',\n",
    "    'omg': 'oh my god',\n",
    "    'af': 'as fuck',\n",
    "    'asap': 'as soon as possible',\n",
    "    'lmao': 'laugh my ass off',\n",
    "    'approx': 'approximately',\n",
    "    'appt': 'appointmnt',\n",
    "    'apt':' apartment',\n",
    "    'dept': 'department',\n",
    "    'diy': 'do it yourself',\n",
    "    'rsvp': 'reserve your spot',\n",
    "    'plz': 'please',\n",
    "    'fk': 'fuck',\n",
    "    'thx': 'thanks',\n",
    "    'u': 'you',\n",
    "    'lol': 'laughing out loud'\n",
    "}\n",
    "\n",
    "\n",
    "def pre_process_text(text):\n",
    "    splitted_text = text.split(\" \")\n",
    "    \n",
    "    result = []\n",
    "\n",
    "    for i in range(len(splitted_text)):\n",
    "        if splitted_text[i] in sad_emoticons:\n",
    "            result.append(\"sad\")\n",
    "        elif splitted_text[i] in happy_emoticons:\n",
    "            result.append(\"happy\")\n",
    "        elif splitted_text[i].startswith(\"#\"):\n",
    "            capital_seperate_text = splitted_text[i][1: ]\n",
    "            for word in re.findall('[A-Z][a-z]*', capital_seperate_text):\n",
    "                result.append(word)\n",
    "        else:\n",
    "            result.append(splitted_text[i])\n",
    "            \n",
    "    result = \" \".join(result)\n",
    "    # mark lower case\n",
    "    lowered = result.translate(str.maketrans('', '', string.punctuation)).lower()\n",
    "\n",
    "    # handle abbreviations from dct above\n",
    "    lowered = \" \".join([dct[i] if i in dct else i for i in lowered.split(\" \")])\n",
    "    # lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(lowered)\n",
    "    tagged = nltk.pos_tag(tokens)\n",
    "    \n",
    "    return \" \".join([lemmatizer.lemmatize(_[0], pos=get_wordnet_pos(_[1])) for _ in tagged])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>withheld_in_countries</th>\n",
       "      <th>user.id</th>\n",
       "      <th>user.name</th>\n",
       "      <th>...</th>\n",
       "      <th>user.listed_count</th>\n",
       "      <th>user.created_at</th>\n",
       "      <th>user.favourites_count</th>\n",
       "      <th>user.statuses_count</th>\n",
       "      <th>user.default_profile</th>\n",
       "      <th>user.default_profile_image</th>\n",
       "      <th>user.withheld_in_countries</th>\n",
       "      <th>date</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>expanded_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.240728e+18</td>\n",
       "      <td>holy shit I hate every person in this video</td>\n",
       "      <td>70104</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.022756e+08</td>\n",
       "      <td>Typreme Kicks.üëü‚ù§Ô∏èüìàü§ôüèΩ</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>Wed Oct 13 17:48:24 +0000 2010</td>\n",
       "      <td>89995</td>\n",
       "      <td>63405</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar 19</td>\n",
       "      <td>-0.5000</td>\n",
       "      <td>holy shit i hate every person in this video</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.240728e+18</td>\n",
       "      <td>There you have it.\\n\\nProof that this is an in...</td>\n",
       "      <td>1662</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.524761e+08</td>\n",
       "      <td>Jake</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Wed Aug 10 17:37:59 +0000 2011</td>\n",
       "      <td>9823</td>\n",
       "      <td>21872</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar 19</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>there you have it proof that this be an intent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.240728e+18</td>\n",
       "      <td>Among other things, what this shows is they're...</td>\n",
       "      <td>6624</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9.281008e+17</td>\n",
       "      <td>Fifty Pound Head</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>Wed Nov 08 03:24:12 +0000 2017</td>\n",
       "      <td>1651</td>\n",
       "      <td>1617</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar 19</td>\n",
       "      <td>-0.0625</td>\n",
       "      <td>among other thing what this show be they be us...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1.240729e+18</td>\n",
       "      <td>this incompetent racist asshole</td>\n",
       "      <td>14445</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.585784e+07</td>\n",
       "      <td>Brad</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>Fri Aug 15 00:33:06 +0000 2008</td>\n",
       "      <td>8538</td>\n",
       "      <td>11553</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar 19</td>\n",
       "      <td>-0.3500</td>\n",
       "      <td>this incompetent racist asshole</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1.240729e+18</td>\n",
       "      <td>Among other things, what this shows is they're...</td>\n",
       "      <td>6624</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.001945e+18</td>\n",
       "      <td>Pamela ( No DMs)</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>Wed May 30 21:56:16 +0000 2018</td>\n",
       "      <td>190490</td>\n",
       "      <td>99423</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mar 19</td>\n",
       "      <td>-0.0625</td>\n",
       "      <td>among other thing what this show be they be us...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1      tweet_id  \\\n",
       "0           0             0  1.240728e+18   \n",
       "1           1             1  1.240728e+18   \n",
       "2           2             2  1.240728e+18   \n",
       "3           3             3  1.240729e+18   \n",
       "4           4             4  1.240729e+18   \n",
       "\n",
       "                                                text  retweet_count  \\\n",
       "0        holy shit I hate every person in this video          70104   \n",
       "1  There you have it.\\n\\nProof that this is an in...           1662   \n",
       "2  Among other things, what this shows is they're...           6624   \n",
       "3                    this incompetent racist asshole          14445   \n",
       "4  Among other things, what this shows is they're...           6624   \n",
       "\n",
       "   favorite_count possibly_sensitive withheld_in_countries       user.id  \\\n",
       "0               0                NaN                   NaN  2.022756e+08   \n",
       "1               0                NaN                   NaN  3.524761e+08   \n",
       "2               0                NaN                   NaN  9.281008e+17   \n",
       "3               0              False                   NaN  1.585784e+07   \n",
       "4               0                NaN                   NaN  1.001945e+18   \n",
       "\n",
       "              user.name  ... user.listed_count  \\\n",
       "0  Typreme Kicks.üëü‚ù§Ô∏èüìàü§ôüèΩ  ...                 9   \n",
       "1                  Jake  ...                 3   \n",
       "2      Fifty Pound Head  ...                 0   \n",
       "3                  Brad  ...                 2   \n",
       "4      Pamela ( No DMs)  ...                 3   \n",
       "\n",
       "                  user.created_at user.favourites_count  user.statuses_count  \\\n",
       "0  Wed Oct 13 17:48:24 +0000 2010                 89995                63405   \n",
       "1  Wed Aug 10 17:37:59 +0000 2011                  9823                21872   \n",
       "2  Wed Nov 08 03:24:12 +0000 2017                  1651                 1617   \n",
       "3  Fri Aug 15 00:33:06 +0000 2008                  8538                11553   \n",
       "4  Wed May 30 21:56:16 +0000 2018                190490                99423   \n",
       "\n",
       "   user.default_profile  user.default_profile_image  \\\n",
       "0                  True                       False   \n",
       "1                 False                       False   \n",
       "2                 False                       False   \n",
       "3                 False                       False   \n",
       "4                  True                       False   \n",
       "\n",
       "  user.withheld_in_countries    date  sentiment_score  \\\n",
       "0                        NaN  Mar 19          -0.5000   \n",
       "1                        NaN  Mar 19           0.0000   \n",
       "2                        NaN  Mar 19          -0.0625   \n",
       "3                        NaN  Mar 19          -0.3500   \n",
       "4                        NaN  Mar 19          -0.0625   \n",
       "\n",
       "                                       expanded_text  \n",
       "0        holy shit i hate every person in this video  \n",
       "1  there you have it proof that this be an intent...  \n",
       "2  among other thing what this show be they be us...  \n",
       "3                    this incompetent racist asshole  \n",
       "4  among other thing what this show be they be us...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"expanded_text\"] = df[\"expanded_text\"].map(pre_process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the preprocessed dataframe (for later evaluation)\n",
    "df.to_csv(PROCESSED_DATA_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python37364bit90982e7f4b014fa3beccbd32e1b2d90e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
