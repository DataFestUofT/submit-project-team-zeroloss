{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\programdata\\anaconda3\\lib\\site-packages (2.11.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (2020.6.8)\n",
      "Requirement already satisfied: packaging in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (19.0)\n",
      "Requirement already satisfied: tokenizers==0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (1.16.4)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (4.32.1)\n",
      "Requirement already satisfied: filelock in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: sentencepiece in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.1.92)\n",
      "Requirement already satisfied: sacremoses in c:\\programdata\\anaconda3\\lib\\site-packages (from transformers) (0.0.43)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (1.24.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2020.4.5.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->transformers) (1.12.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from packaging->transformers) (2.4.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (7.0)\n",
      "Requirement already satisfied: joblib in c:\\programdata\\anaconda3\\lib\\site-packages (from sacremoses->transformers) (0.13.2)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.4.4)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install transformers\n",
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils import data\n",
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "\n",
    "SEED = 412413\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"final_train_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "df = df.sample(frac=1)\n",
    "\n",
    "train_num = math.ceil(0.7 * len(df))\n",
    "valid_num = math.ceil(0.9 * len(df))\n",
    "train_data = df.iloc[:train_num, :].reset_index()\n",
    "valid_data = df.iloc[train_num:valid_num, :].reset_index()\n",
    "test_data = df.iloc[valid_num:, :].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.1568, 0.4638, 0.3794])\n"
     ]
    }
   ],
   "source": [
    "args = {\n",
    "    'batch_size': 256,\n",
    "    'lr': 3e-4,\n",
    "    'hidden_dim': 128,\n",
    "    'n_layers': 1,\n",
    "    'bidirectional': True,\n",
    "    'dropout': 0.2,\n",
    "    'n_epochs': 20,\n",
    "    'b1': 0.9,\n",
    "    'b2': 0.999,\n",
    "    'weight_decay': 0.01,\n",
    "    'lr_decay': 0.7\n",
    "}\n",
    "\n",
    "num_positive = (df[\"sentiment\"] == \"positive\").sum()\n",
    "num_negative = (df[\"sentiment\"] == \"negative\").sum()\n",
    "num_neutral = (df[\"sentiment\"] == \"neutral\").sum()\n",
    "\n",
    "args[\"weight\"] = torch.tensor([num_negative / len(df), num_neutral / len(df), num_positive / len(df)], dtype=torch.float32)\n",
    "\n",
    "print(args[\"weight\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tokenized_train = train_data['text'].apply((\n",
    "    lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized_valid = valid_data['text'].apply((\n",
    "    lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "tokenized_test = test_data['text'].apply((\n",
    "    lambda x: tokenizer.encode(x, add_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(tokenized):\n",
    "    max_len = 0\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n",
      "59\n",
      "45\n"
     ]
    }
   ],
   "source": [
    "max_len_train = get_max_len(tokenized_train)\n",
    "print(max_len_train)\n",
    "max_len_valid = get_max_len(tokenized_valid)\n",
    "print(max_len_valid)\n",
    "max_len_test = get_max_len(tokenized_test)\n",
    "print(max_len_test)\n",
    "max_len = max([max_len_train, max_len_valid, max_len_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_train = torch.tensor([i + [0] * (max_len - len(i)) \n",
    "                             for i in tokenized_train.values])\n",
    "padded_valid = torch.tensor([i + [0] * (max_len - len(i)) \n",
    "                             for i in tokenized_valid.values])\n",
    "padded_test = torch.tensor([i + [0] * (max_len - len(i)) \n",
    "                            for i in tokenized_test.values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = torch.tensor(train_data['sentiment'].replace(\n",
    "    to_replace='positive', value=2).replace(\n",
    "    to_replace='negative', value=0).replace(\n",
    "    to_replace='neutral', value=1))\n",
    "valid_label = torch.tensor(valid_data['sentiment'].replace(\n",
    "    to_replace='positive', value=2).replace(\n",
    "    to_replace='negative', value=0).replace(\n",
    "    to_replace='neutral', value=1))\n",
    "test_label = torch.tensor(test_data['sentiment'].replace(\n",
    "    to_replace='positive', value=2).replace(\n",
    "    to_replace='negative', value=0).replace(\n",
    "    to_replace='neutral', value=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and data iterators\n",
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, x, labels):\n",
    "        'Initialization'\n",
    "        self.x = x\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        # Load data and get label\n",
    "        x = self.x[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Dataset(padded_train, train_label)\n",
    "validset = Dataset(padded_valid, valid_label)\n",
    "testset = Dataset(padded_test, test_label)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(trainset,\n",
    "                                           batch_size=args['batch_size'],\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "valid_loader = torch.utils.data.DataLoader(validset,\n",
    "                                           batch_size=args['batch_size'],\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(testset,\n",
    "                                           batch_size=args['batch_size'],\n",
    "                                           shuffle=True,\n",
    "                                           drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(trainset, \"trainset.pt\")\n",
    "# torch.save(validset, \"validset.pt\")\n",
    "# torch.save(testset, \"testset.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "#         embedding_dim = bert.config.to_dict()['dim']\n",
    "        embedding_dim = 768\n",
    "    \n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        attention_mask = text.masked_fill(text != 0, 1)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text, attention_mask=attention_mask)[0]\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        _, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTLSTMSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "#         embedding_dim = bert.config.to_dict()['dim']\n",
    "        embedding_dim = 768\n",
    "    \n",
    "        self.rnn = nn.LSTM(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        attention_mask = text.masked_fill(text != 0, 1)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text, attention_mask=attention_mask)[0]\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        _, (hidden, _) = self.rnn(embedded)\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        # new activation\n",
    "        sig_out = self.tanh(output)\n",
    "        \n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return sig_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = RobertaModel.from_pretrained('roberta-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = BERTGRUSentiment(bert,\n",
    "#                          args['hidden_dim'],\n",
    "#                          3,\n",
    "#                          args['n_layers'],\n",
    "#                          args['bidirectional'],\n",
    "#                          args['dropout']).to(device)\n",
    "model = BERTLSTMSentiment(bert,\n",
    "                         args['hidden_dim'],\n",
    "                         3,\n",
    "                         args['n_layers'],\n",
    "                         args['bidirectional'],\n",
    "                         args['dropout']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if name.startswith('bert'):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 920,323 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rnn.weight_ih_l0\n",
      "rnn.weight_hh_l0\n",
      "rnn.bias_ih_l0\n",
      "rnn.bias_hh_l0\n",
      "rnn.weight_ih_l0_reverse\n",
      "rnn.weight_hh_l0_reverse\n",
      "rnn.bias_ih_l0_reverse\n",
      "rnn.bias_hh_l0_reverse\n",
      "out.weight\n",
      "out.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():                \n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = optim.Adam(model.parameters(), \n",
    "#                        lr=args['lr'], \n",
    "#                        betas=(args[\"b1\"], args[\"b2\"]),\n",
    "#                        weight_decay=args[\"weight_decay\"])\n",
    "\n",
    "optimizer = optim.AdamW(model.parameters(), \n",
    "                       lr=args['lr'], \n",
    "                       betas=(args[\"b1\"], args[\"b2\"]),\n",
    "                       weight_decay=args[\"weight_decay\"])\n",
    "\n",
    "# optimizer = optim.SGD(model.parameters(), momentum=0.9, lr=args[\"lr\"])\n",
    "\n",
    "scheduler = MultiStepLR(optimizer, milestones=[20, 40], gamma=args[\"lr_decay\"])\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(weight=args['weight']).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_label):\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    y_pred_softmax = softmax(y_pred)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
    "#     print(y_pred_tags)\n",
    "\n",
    "    # accu\n",
    "    correct_pred = (y_pred_tags == y_label).float()\n",
    "    acc = correct_pred.sum() / len(y_label)\n",
    "\n",
    "    # roc-auc\n",
    "    one_hot_label = nn.functional.one_hot(y_label)\n",
    "#     roc_auc = roc_auc_score(one_hot_label.detach().cpu(), y_pred_softmax.detach().cpu(), average=\"macro\")\n",
    "\n",
    "    # f1\n",
    "    f1 = f1_score(y_label.detach().cpu(), y_pred_tags.detach().cpu(), average='weighted')\n",
    "    \n",
    "    return acc, 1, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_rocauc = 0\n",
    "    epoch_f1 = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(data).squeeze(1)\n",
    "\n",
    "        loss = criterion(predictions, target)\n",
    "        \n",
    "        acc, roc_auc, f1 = multi_acc(predictions, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_rocauc += roc_auc\n",
    "        epoch_f1 += f1\n",
    "\n",
    "        print(\"batch idx {}: | train loss: {} | train accu: {:.3f} | train roc: {:.3f} | train f1: {}\".format(\n",
    "            batch_idx, loss.item(), acc.item(), roc_auc, f1))\n",
    "        \n",
    "    return epoch_loss / len(data_loader), epoch_acc / len(data_loader), epoch_rocauc / len(data_loader), epoch_f1 / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_rocauc = 0\n",
    "    epoch_f1 = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            predictions = model(data).squeeze(1)\n",
    "            loss = criterion(predictions, target)\n",
    "            \n",
    "            acc, roc_auc, f1 = multi_acc(predictions, target)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_rocauc += roc_auc\n",
    "            epoch_f1 += f1\n",
    "        \n",
    "    return epoch_loss / len(data_loader), epoch_acc / len(data_loader), epoch_rocauc / len(data_loader), epoch_f1 / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx 0: | train loss: 1.127386450767517 | train accu: 0.211 | train roc: 1.000 | train f1: 0.23354078618250818\n",
      "batch idx 1: | train loss: 1.0826241970062256 | train accu: 0.398 | train roc: 1.000 | train f1: 0.3911313657407407\n",
      "batch idx 2: | train loss: 1.0369826555252075 | train accu: 0.484 | train roc: 1.000 | train f1: 0.435211879645169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1437: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx 3: | train loss: 1.010359764099121 | train accu: 0.457 | train roc: 1.000 | train f1: 0.40250075665859564\n",
      "batch idx 4: | train loss: 0.9804185628890991 | train accu: 0.469 | train roc: 1.000 | train f1: 0.382580974544073\n",
      "batch idx 5: | train loss: 0.9657912850379944 | train accu: 0.465 | train roc: 1.000 | train f1: 0.3555530954790823\n",
      "batch idx 6: | train loss: 0.9570099115371704 | train accu: 0.426 | train roc: 1.000 | train f1: 0.2890475335249042\n",
      "batch idx 7: | train loss: 0.9345581531524658 | train accu: 0.434 | train roc: 1.000 | train f1: 0.27606012040595723\n",
      "batch idx 8: | train loss: 0.9311413764953613 | train accu: 0.461 | train roc: 1.000 | train f1: 0.3054148092473564\n",
      "batch idx 9: | train loss: 0.9040330648422241 | train accu: 0.469 | train roc: 1.000 | train f1: 0.3057107528851438\n",
      "batch idx 10: | train loss: 0.9125540852546692 | train accu: 0.465 | train roc: 1.000 | train f1: 0.2975\n",
      "batch idx 11: | train loss: 0.8747515678405762 | train accu: 0.488 | train roc: 1.000 | train f1: 0.3212376644736842\n",
      "batch idx 12: | train loss: 0.9159205555915833 | train accu: 0.430 | train roc: 1.000 | train f1: 0.2582821038251366\n",
      "batch idx 13: | train loss: 0.8869932889938354 | train accu: 0.453 | train roc: 1.000 | train f1: 0.2825940860215054\n",
      "batch idx 14: | train loss: 0.9063695073127747 | train accu: 0.430 | train roc: 1.000 | train f1: 0.2582821038251366\n",
      "batch idx 15: | train loss: 0.875476062297821 | train accu: 0.453 | train roc: 1.000 | train f1: 0.2825940860215054\n",
      "batch idx 16: | train loss: 0.8717557787895203 | train accu: 0.473 | train roc: 1.000 | train f1: 0.30340268567639256\n",
      "batch idx 17: | train loss: 0.8802720904350281 | train accu: 0.453 | train roc: 1.000 | train f1: 0.2825940860215054\n",
      "batch idx 18: | train loss: 0.8983974456787109 | train accu: 0.410 | train roc: 1.000 | train f1: 0.2385950484764543\n",
      "batch idx 19: | train loss: 0.914348840713501 | train accu: 0.391 | train roc: 1.000 | train f1: 0.21945224719101125\n",
      "batch idx 20: | train loss: 0.8750384449958801 | train accu: 0.449 | train roc: 1.000 | train f1: 0.27849140835579517\n",
      "batch idx 21: | train loss: 0.864814817905426 | train accu: 0.453 | train roc: 1.000 | train f1: 0.2825940860215054\n",
      "batch idx 22: | train loss: 0.8758968710899353 | train accu: 0.457 | train roc: 1.000 | train f1: 0.2867166554959785\n",
      "batch idx 23: | train loss: 0.8821691274642944 | train accu: 0.465 | train roc: 1.000 | train f1: 0.29502083333333334\n",
      "batch idx 24: | train loss: 0.8907894492149353 | train accu: 0.473 | train roc: 1.000 | train f1: 0.30340268567639256\n",
      "batch idx 25: | train loss: 0.8522402048110962 | train accu: 0.512 | train roc: 1.000 | train f1: 0.3464349160206719\n",
      "batch idx 26: | train loss: 0.8933278322219849 | train accu: 0.434 | train roc: 1.000 | train f1: 0.26228286784741145\n",
      "batch idx 27: | train loss: 0.8830780386924744 | train accu: 0.465 | train roc: 1.000 | train f1: 0.29502083333333334\n",
      "batch idx 28: | train loss: 0.8678184151649475 | train accu: 0.488 | train roc: 1.000 | train f1: 0.3203945209973753\n",
      "batch idx 29: | train loss: 0.879743218421936 | train accu: 0.406 | train roc: 1.000 | train f1: 0.2347222222222222\n",
      "batch idx 30: | train loss: 0.8217995166778564 | train accu: 0.500 | train roc: 1.000 | train f1: 0.3333333333333333\n",
      "batch idx 31: | train loss: 0.8768320083618164 | train accu: 0.434 | train roc: 1.000 | train f1: 0.26228286784741145\n",
      "batch idx 32: | train loss: 0.8588753938674927 | train accu: 0.484 | train roc: 1.000 | train f1: 0.3161184210526316\n",
      "batch idx 33: | train loss: 0.867918848991394 | train accu: 0.461 | train roc: 1.000 | train f1: 0.2908589572192513\n",
      "batch idx 34: | train loss: 0.8623480796813965 | train accu: 0.469 | train roc: 1.000 | train f1: 0.2992021276595745\n",
      "batch idx 35: | train loss: 0.8608035445213318 | train accu: 0.496 | train roc: 1.000 | train f1: 0.3290021214099217\n",
      "batch idx 36: | train loss: 0.8687300086021423 | train accu: 0.484 | train roc: 1.000 | train f1: 0.3161184210526316\n",
      "batch idx 37: | train loss: 0.8702172040939331 | train accu: 0.426 | train roc: 1.000 | train f1: 0.25430222602739727\n",
      "batch idx 38: | train loss: 0.8025863766670227 | train accu: 0.523 | train roc: 1.000 | train f1: 0.3596955128205128\n",
      "batch idx 39: | train loss: 0.8452057838439941 | train accu: 0.484 | train roc: 1.000 | train f1: 0.3161184210526316\n",
      "batch idx 40: | train loss: 0.8450441360473633 | train accu: 0.488 | train roc: 1.000 | train f1: 0.3203945209973753\n",
      "batch idx 41: | train loss: 0.8488550186157227 | train accu: 0.492 | train roc: 1.000 | train f1: 0.3246891361256545\n",
      "batch idx 42: | train loss: 0.8787127733230591 | train accu: 0.418 | train roc: 1.000 | train f1: 0.24640581955922863\n",
      "batch idx 43: | train loss: 0.8526544570922852 | train accu: 0.441 | train roc: 1.000 | train f1: 0.2703463753387534\n",
      "batch idx 44: | train loss: 0.8835265636444092 | train accu: 0.426 | train roc: 1.000 | train f1: 0.25430222602739727\n",
      "batch idx 45: | train loss: 0.9047307372093201 | train accu: 0.391 | train roc: 1.000 | train f1: 0.21945224719101125\n",
      "batch idx 46: | train loss: 0.8605876564979553 | train accu: 0.469 | train roc: 1.000 | train f1: 0.2992021276595745\n",
      "batch idx 47: | train loss: 0.8714233040809631 | train accu: 0.438 | train roc: 1.000 | train f1: 0.266304347826087\n",
      "batch idx 48: | train loss: 0.8648002743721008 | train accu: 0.477 | train roc: 1.000 | train f1: 0.3076223544973545\n",
      "batch idx 49: | train loss: 0.8474317789077759 | train accu: 0.473 | train roc: 1.000 | train f1: 0.30340268567639256\n",
      "batch idx 50: | train loss: 0.8649555444717407 | train accu: 0.480 | train roc: 1.000 | train f1: 0.31186098284960423\n",
      "batch idx 51: | train loss: 0.8569799065589905 | train accu: 0.441 | train roc: 1.000 | train f1: 0.2703463753387534\n",
      "batch idx 52: | train loss: 0.8856202363967896 | train accu: 0.441 | train roc: 1.000 | train f1: 0.2703463753387534\n",
      "batch idx 53: | train loss: 0.8905283212661743 | train accu: 0.402 | train roc: 1.000 | train f1: 0.23087134401114207\n",
      "batch idx 54: | train loss: 0.8648133277893066 | train accu: 0.453 | train roc: 1.000 | train f1: 0.283355795148248\n",
      "batch idx 55: | train loss: 0.8596602082252502 | train accu: 0.441 | train roc: 1.000 | train f1: 0.2703463753387534\n",
      "batch idx 56: | train loss: 0.8680844902992249 | train accu: 0.465 | train roc: 1.000 | train f1: 0.29502083333333334\n",
      "batch idx 57: | train loss: 0.8600122928619385 | train accu: 0.445 | train roc: 1.000 | train f1: 0.2744087837837838\n",
      "batch idx 58: | train loss: 0.8746212720870972 | train accu: 0.441 | train roc: 1.000 | train f1: 0.2703463753387534\n",
      "batch idx 59: | train loss: 0.882209062576294 | train accu: 0.418 | train roc: 1.000 | train f1: 0.24640581955922863\n",
      "batch idx 60: | train loss: 0.8745356798171997 | train accu: 0.438 | train roc: 1.000 | train f1: 0.266304347826087\n",
      "batch idx 61: | train loss: 0.8358517289161682 | train accu: 0.488 | train roc: 1.000 | train f1: 0.3203945209973753\n",
      "batch idx 62: | train loss: 0.8550849556922913 | train accu: 0.453 | train roc: 1.000 | train f1: 0.2825940860215054\n",
      "batch idx 63: | train loss: 0.8590275049209595 | train accu: 0.477 | train roc: 1.000 | train f1: 0.3076223544973545\n",
      "batch idx 64: | train loss: 0.8847525119781494 | train accu: 0.449 | train roc: 1.000 | train f1: 0.27849140835579517\n",
      "batch idx 65: | train loss: 0.8852672576904297 | train accu: 0.426 | train roc: 1.000 | train f1: 0.25430222602739727\n",
      "batch idx 66: | train loss: 0.8457244634628296 | train accu: 0.500 | train roc: 1.000 | train f1: 0.3333333333333333\n",
      "batch idx 67: | train loss: 0.8420901894569397 | train accu: 0.492 | train roc: 1.000 | train f1: 0.3246891361256545\n",
      "batch idx 68: | train loss: 0.8432554602622986 | train accu: 0.477 | train roc: 1.000 | train f1: 0.3076223544973545\n",
      "batch idx 69: | train loss: 0.8831884264945984 | train accu: 0.441 | train roc: 1.000 | train f1: 0.2703463753387534\n",
      "batch idx 70: | train loss: 0.8574122786521912 | train accu: 0.430 | train roc: 1.000 | train f1: 0.2582821038251366\n",
      "batch idx 71: | train loss: 0.878648042678833 | train accu: 0.438 | train roc: 1.000 | train f1: 0.266304347826087\n",
      "batch idx 72: | train loss: 0.8412160873413086 | train accu: 0.461 | train roc: 1.000 | train f1: 0.2908589572192513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx 73: | train loss: 0.8304031491279602 | train accu: 0.516 | train roc: 1.000 | train f1: 0.3508376288659794\n",
      "batch idx 74: | train loss: 0.8363572359085083 | train accu: 0.480 | train roc: 1.000 | train f1: 0.31186098284960423\n",
      "batch idx 75: | train loss: 0.8369020223617554 | train accu: 0.504 | train roc: 1.000 | train f1: 0.33768262987012987\n",
      "batch idx 76: | train loss: 0.8463298082351685 | train accu: 0.516 | train roc: 1.000 | train f1: 0.3508376288659794\n",
      "batch idx 77: | train loss: 0.83852219581604 | train accu: 0.492 | train roc: 1.000 | train f1: 0.3246891361256545\n",
      "batch idx 78: | train loss: 0.8190856575965881 | train accu: 0.500 | train roc: 1.000 | train f1: 0.3333333333333333\n",
      "batch idx 79: | train loss: 0.9012531042098999 | train accu: 0.387 | train roc: 1.000 | train f1: 0.21569102112676058\n",
      "batch idx 80: | train loss: 0.8430414199829102 | train accu: 0.477 | train roc: 1.000 | train f1: 0.3076223544973545\n",
      "batch idx 81: | train loss: 0.8541215658187866 | train accu: 0.473 | train roc: 1.000 | train f1: 0.30340268567639256\n",
      "batch idx 82: | train loss: 0.8626734018325806 | train accu: 0.465 | train roc: 1.000 | train f1: 0.29502083333333334\n",
      "batch idx 83: | train loss: 0.8614362478256226 | train accu: 0.449 | train roc: 1.000 | train f1: 0.27849140835579517\n",
      "batch idx 84: | train loss: 0.8437562584877014 | train accu: 0.512 | train roc: 1.000 | train f1: 0.35065780573593075\n",
      "batch idx 85: | train loss: 0.8825157284736633 | train accu: 0.414 | train roc: 1.000 | train f1: 0.24248964088397787\n",
      "batch idx 86: | train loss: 0.8396157622337341 | train accu: 0.473 | train roc: 1.000 | train f1: 0.30340268567639256\n",
      "batch idx 87: | train loss: 0.8701988458633423 | train accu: 0.449 | train roc: 1.000 | train f1: 0.27924408783783783\n",
      "batch idx 88: | train loss: 0.873410701751709 | train accu: 0.438 | train roc: 1.000 | train f1: 0.266304347826087\n",
      "batch idx 89: | train loss: 0.8471276164054871 | train accu: 0.453 | train roc: 1.000 | train f1: 0.2825940860215054\n",
      "batch idx 90: | train loss: 0.853489339351654 | train accu: 0.453 | train roc: 1.000 | train f1: 0.2850302419354839\n",
      "batch idx 91: | train loss: 0.8663437366485596 | train accu: 0.453 | train roc: 1.000 | train f1: 0.2825940860215054\n",
      "batch idx 92: | train loss: 0.8554584980010986 | train accu: 0.457 | train roc: 1.000 | train f1: 0.2867166554959785\n",
      "batch idx 93: | train loss: 0.8702067732810974 | train accu: 0.434 | train roc: 1.000 | train f1: 0.26464577656675753\n",
      "batch idx 94: | train loss: 0.8242703080177307 | train accu: 0.504 | train roc: 1.000 | train f1: 0.34446089800566637\n",
      "batch idx 95: | train loss: 0.839247465133667 | train accu: 0.480 | train roc: 1.000 | train f1: 0.31186098284960423\n",
      "batch idx 96: | train loss: 0.8296873569488525 | train accu: 0.480 | train roc: 1.000 | train f1: 0.31616771721033915\n",
      "batch idx 97: | train loss: 0.8455379009246826 | train accu: 0.461 | train roc: 1.000 | train f1: 0.2908589572192513\n",
      "batch idx 98: | train loss: 0.8469477891921997 | train accu: 0.457 | train roc: 1.000 | train f1: 0.2934563145499186\n",
      "batch idx 99: | train loss: 0.8840969800949097 | train accu: 0.449 | train roc: 1.000 | train f1: 0.28287616061529935\n",
      "batch idx 100: | train loss: 0.8123348355293274 | train accu: 0.516 | train roc: 1.000 | train f1: 0.3697629574962773\n",
      "batch idx 101: | train loss: 0.8571471571922302 | train accu: 0.441 | train roc: 1.000 | train f1: 0.27906601638943246\n",
      "batch idx 102: | train loss: 0.8325327038764954 | train accu: 0.492 | train roc: 1.000 | train f1: 0.333079983785629\n",
      "batch idx 103: | train loss: 0.8514484167098999 | train accu: 0.484 | train roc: 1.000 | train f1: 0.335421668667467\n",
      "batch idx 104: | train loss: 0.8436974287033081 | train accu: 0.461 | train roc: 1.000 | train f1: 0.31241708431603776\n",
      "batch idx 105: | train loss: 0.8677473068237305 | train accu: 0.438 | train roc: 1.000 | train f1: 0.2971403368196131\n",
      "batch idx 106: | train loss: 0.8324558734893799 | train accu: 0.496 | train roc: 1.000 | train f1: 0.3562505530371097\n",
      "batch idx 107: | train loss: 0.8109357953071594 | train accu: 0.480 | train roc: 1.000 | train f1: 0.3404644151954603\n",
      "batch idx 108: | train loss: 0.8415402173995972 | train accu: 0.453 | train roc: 1.000 | train f1: 0.32398258911546307\n",
      "batch idx 109: | train loss: 0.8261972069740295 | train accu: 0.465 | train roc: 1.000 | train f1: 0.3465678418803419\n",
      "batch idx 110: | train loss: 0.8626770377159119 | train accu: 0.473 | train roc: 1.000 | train f1: 0.3488992606529574\n",
      "batch idx 111: | train loss: 0.8626908659934998 | train accu: 0.469 | train roc: 1.000 | train f1: 0.3391962418257393\n",
      "batch idx 112: | train loss: 0.8542352318763733 | train accu: 0.465 | train roc: 1.000 | train f1: 0.343497143175452\n",
      "batch idx 113: | train loss: 0.8328817486763 | train accu: 0.496 | train roc: 1.000 | train f1: 0.37249410377358494\n",
      "batch idx 114: | train loss: 0.8611996173858643 | train accu: 0.469 | train roc: 1.000 | train f1: 0.3225286698764583\n",
      "batch idx 115: | train loss: 0.8440979719161987 | train accu: 0.465 | train roc: 1.000 | train f1: 0.3298846080183861\n",
      "batch idx 116: | train loss: 0.8213255405426025 | train accu: 0.492 | train roc: 1.000 | train f1: 0.3855313387784091\n",
      "batch idx 117: | train loss: 0.7954106330871582 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4626724699516409\n",
      "batch idx 118: | train loss: 0.8733698725700378 | train accu: 0.430 | train roc: 1.000 | train f1: 0.3263794318616208\n",
      "batch idx 119: | train loss: 0.8211067318916321 | train accu: 0.566 | train roc: 1.000 | train f1: 0.49980674087816956\n",
      "Epoch: 01 | Epoch Time: 2m 11s\n",
      "\tTrain Loss: 0.871 | Train Acc: 46.02 | Train rocauc: 1.0 | Train f1: 0.30431541350252117%\n",
      "\t Val. Loss: 0.832 |  Val. Acc: 52.90 | Val. rocauc: 1.0 | Val. f1: 0.46139017242391345%\n",
      "batch idx 0: | train loss: 0.8531395196914673 | train accu: 0.500 | train roc: 1.000 | train f1: 0.43137021245390544\n",
      "batch idx 1: | train loss: 0.8334131240844727 | train accu: 0.488 | train roc: 1.000 | train f1: 0.4201369282891022\n",
      "batch idx 2: | train loss: 0.8536988496780396 | train accu: 0.488 | train roc: 1.000 | train f1: 0.4204727564102564\n",
      "batch idx 3: | train loss: 0.87577223777771 | train accu: 0.445 | train roc: 1.000 | train f1: 0.34625359925474253\n",
      "batch idx 4: | train loss: 0.8297967910766602 | train accu: 0.539 | train roc: 1.000 | train f1: 0.43957631659522356\n",
      "batch idx 5: | train loss: 0.832862377166748 | train accu: 0.539 | train roc: 1.000 | train f1: 0.44527361147498035\n",
      "batch idx 6: | train loss: 0.78499436378479 | train accu: 0.543 | train roc: 1.000 | train f1: 0.43205634550912214\n",
      "batch idx 7: | train loss: 0.8504959344863892 | train accu: 0.488 | train roc: 1.000 | train f1: 0.383928990060015\n",
      "batch idx 8: | train loss: 0.8266618251800537 | train accu: 0.516 | train roc: 1.000 | train f1: 0.40500479634002357\n",
      "batch idx 9: | train loss: 0.8512516021728516 | train accu: 0.480 | train roc: 1.000 | train f1: 0.3626852876106195\n",
      "batch idx 10: | train loss: 0.8822078704833984 | train accu: 0.465 | train roc: 1.000 | train f1: 0.3509479112157684\n",
      "batch idx 11: | train loss: 0.8157894611358643 | train accu: 0.492 | train roc: 1.000 | train f1: 0.4147647090751194\n",
      "batch idx 12: | train loss: 0.8335592150688171 | train accu: 0.543 | train roc: 1.000 | train f1: 0.4850779923599321\n",
      "batch idx 13: | train loss: 0.8444926142692566 | train accu: 0.535 | train roc: 1.000 | train f1: 0.48110183189655176\n",
      "batch idx 14: | train loss: 0.8278782963752747 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5113643950033627\n",
      "batch idx 15: | train loss: 0.8273925185203552 | train accu: 0.480 | train roc: 1.000 | train f1: 0.4283124054792927\n",
      "batch idx 16: | train loss: 0.8359039425849915 | train accu: 0.562 | train roc: 1.000 | train f1: 0.49987019730010374\n",
      "batch idx 17: | train loss: 0.8322184085845947 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4772727272727273\n",
      "batch idx 18: | train loss: 0.8373532891273499 | train accu: 0.480 | train roc: 1.000 | train f1: 0.40660338785046735\n",
      "batch idx 19: | train loss: 0.8606888055801392 | train accu: 0.500 | train roc: 1.000 | train f1: 0.4118640374768493\n",
      "batch idx 20: | train loss: 0.7933872938156128 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4545498927262123\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx 21: | train loss: 0.8586851954460144 | train accu: 0.477 | train roc: 1.000 | train f1: 0.3887674307841457\n",
      "batch idx 22: | train loss: 0.8371204137802124 | train accu: 0.520 | train roc: 1.000 | train f1: 0.442697874832664\n",
      "batch idx 23: | train loss: 0.8343857526779175 | train accu: 0.484 | train roc: 1.000 | train f1: 0.41476969430410204\n",
      "batch idx 24: | train loss: 0.8332566022872925 | train accu: 0.562 | train roc: 1.000 | train f1: 0.49880930573365084\n",
      "batch idx 25: | train loss: 0.7935106158256531 | train accu: 0.535 | train roc: 1.000 | train f1: 0.4783159859572784\n",
      "batch idx 26: | train loss: 0.8142514228820801 | train accu: 0.520 | train roc: 1.000 | train f1: 0.47117141812865493\n",
      "batch idx 27: | train loss: 0.906700849533081 | train accu: 0.438 | train roc: 1.000 | train f1: 0.3898426120348827\n",
      "batch idx 28: | train loss: 0.8478903770446777 | train accu: 0.543 | train roc: 1.000 | train f1: 0.4891915327288294\n",
      "batch idx 29: | train loss: 0.8386271595954895 | train accu: 0.543 | train roc: 1.000 | train f1: 0.48800077041602463\n",
      "batch idx 30: | train loss: 0.796062707901001 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5020083507566706\n",
      "batch idx 31: | train loss: 0.8292392492294312 | train accu: 0.543 | train roc: 1.000 | train f1: 0.4650088028169015\n",
      "batch idx 32: | train loss: 0.8780910968780518 | train accu: 0.480 | train roc: 1.000 | train f1: 0.39855346706062833\n",
      "batch idx 33: | train loss: 0.8369945883750916 | train accu: 0.523 | train roc: 1.000 | train f1: 0.4408894230769231\n",
      "batch idx 34: | train loss: 0.8499306440353394 | train accu: 0.473 | train roc: 1.000 | train f1: 0.39551237978394044\n",
      "batch idx 35: | train loss: 0.7869032025337219 | train accu: 0.594 | train roc: 1.000 | train f1: 0.53806258790436\n",
      "batch idx 36: | train loss: 0.8756219148635864 | train accu: 0.500 | train roc: 1.000 | train f1: 0.4526144566869301\n",
      "batch idx 37: | train loss: 0.8372877240180969 | train accu: 0.531 | train roc: 1.000 | train f1: 0.48705335698185714\n",
      "batch idx 38: | train loss: 0.885098397731781 | train accu: 0.488 | train roc: 1.000 | train f1: 0.4276529947916667\n",
      "batch idx 39: | train loss: 0.8369133472442627 | train accu: 0.508 | train roc: 1.000 | train f1: 0.458701669357275\n",
      "batch idx 40: | train loss: 0.8139134645462036 | train accu: 0.535 | train roc: 1.000 | train f1: 0.47883103649068326\n",
      "batch idx 41: | train loss: 0.8371178507804871 | train accu: 0.520 | train roc: 1.000 | train f1: 0.45994274068322977\n",
      "batch idx 42: | train loss: 0.8197954893112183 | train accu: 0.516 | train roc: 1.000 | train f1: 0.45472687752016133\n",
      "batch idx 43: | train loss: 0.8504658937454224 | train accu: 0.484 | train roc: 1.000 | train f1: 0.3969351406855376\n",
      "batch idx 44: | train loss: 0.8899330496788025 | train accu: 0.430 | train roc: 1.000 | train f1: 0.33261043604141083\n",
      "batch idx 45: | train loss: 0.8383793234825134 | train accu: 0.527 | train roc: 1.000 | train f1: 0.46136196100986554\n",
      "batch idx 46: | train loss: 0.8209682703018188 | train accu: 0.555 | train roc: 1.000 | train f1: 0.5016251184101146\n",
      "batch idx 47: | train loss: 0.8122320175170898 | train accu: 0.551 | train roc: 1.000 | train f1: 0.48283815178110484\n",
      "batch idx 48: | train loss: 0.8027773499488831 | train accu: 0.551 | train roc: 1.000 | train f1: 0.49635898629972364\n",
      "batch idx 49: | train loss: 0.8480722308158875 | train accu: 0.527 | train roc: 1.000 | train f1: 0.4693858729419075\n",
      "batch idx 50: | train loss: 0.8739280104637146 | train accu: 0.508 | train roc: 1.000 | train f1: 0.4522914739438792\n",
      "batch idx 51: | train loss: 0.8406788110733032 | train accu: 0.492 | train roc: 1.000 | train f1: 0.43929521615965694\n",
      "batch idx 52: | train loss: 0.7955424189567566 | train accu: 0.551 | train roc: 1.000 | train f1: 0.5040814472591362\n",
      "batch idx 53: | train loss: 0.7862924933433533 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5295517825502833\n",
      "batch idx 54: | train loss: 0.76942378282547 | train accu: 0.551 | train roc: 1.000 | train f1: 0.49200856587327507\n",
      "batch idx 55: | train loss: 0.8082200288772583 | train accu: 0.527 | train roc: 1.000 | train f1: 0.4629916277258567\n",
      "batch idx 56: | train loss: 0.8781177401542664 | train accu: 0.457 | train roc: 1.000 | train f1: 0.3810680338472349\n",
      "batch idx 57: | train loss: 0.861416220664978 | train accu: 0.465 | train roc: 1.000 | train f1: 0.39860340250965254\n",
      "batch idx 58: | train loss: 0.7834582328796387 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5285064088021778\n",
      "batch idx 59: | train loss: 0.8269092440605164 | train accu: 0.531 | train roc: 1.000 | train f1: 0.47699424342105257\n",
      "batch idx 60: | train loss: 0.8648026585578918 | train accu: 0.469 | train roc: 1.000 | train f1: 0.43153903337475585\n",
      "batch idx 61: | train loss: 0.8091292381286621 | train accu: 0.551 | train roc: 1.000 | train f1: 0.5114100998300766\n",
      "batch idx 62: | train loss: 0.8486853837966919 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4881008144242538\n",
      "batch idx 63: | train loss: 0.8427834510803223 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4920575464774951\n",
      "batch idx 64: | train loss: 0.8055176734924316 | train accu: 0.535 | train roc: 1.000 | train f1: 0.49534466094617163\n",
      "batch idx 65: | train loss: 0.7657603025436401 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5442187723118663\n",
      "batch idx 66: | train loss: 0.8132203817367554 | train accu: 0.551 | train roc: 1.000 | train f1: 0.5047906839622641\n",
      "batch idx 67: | train loss: 0.8053290247917175 | train accu: 0.543 | train roc: 1.000 | train f1: 0.498734067390207\n",
      "batch idx 68: | train loss: 0.8075109720230103 | train accu: 0.547 | train roc: 1.000 | train f1: 0.49592956720765713\n",
      "batch idx 69: | train loss: 0.8275904059410095 | train accu: 0.523 | train roc: 1.000 | train f1: 0.4769136139340353\n",
      "batch idx 70: | train loss: 0.8640137314796448 | train accu: 0.500 | train roc: 1.000 | train f1: 0.4528636759581881\n",
      "batch idx 71: | train loss: 0.8051654100418091 | train accu: 0.535 | train roc: 1.000 | train f1: 0.487947071697695\n",
      "batch idx 72: | train loss: 0.806692361831665 | train accu: 0.547 | train roc: 1.000 | train f1: 0.49526617250673854\n",
      "batch idx 73: | train loss: 0.8368374109268188 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5272623954286114\n",
      "batch idx 74: | train loss: 0.8215882182121277 | train accu: 0.539 | train roc: 1.000 | train f1: 0.49551799886621317\n",
      "batch idx 75: | train loss: 0.8375346064567566 | train accu: 0.520 | train roc: 1.000 | train f1: 0.4733506141284234\n",
      "batch idx 76: | train loss: 0.8295288681983948 | train accu: 0.535 | train roc: 1.000 | train f1: 0.483242567314884\n",
      "batch idx 77: | train loss: 0.8591214418411255 | train accu: 0.488 | train roc: 1.000 | train f1: 0.4272991071428571\n",
      "batch idx 78: | train loss: 0.8828057050704956 | train accu: 0.480 | train roc: 1.000 | train f1: 0.4249138959439698\n",
      "batch idx 79: | train loss: 0.84324711561203 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4977159232777211\n",
      "batch idx 80: | train loss: 0.8307024240493774 | train accu: 0.570 | train roc: 1.000 | train f1: 0.49949415467625896\n",
      "batch idx 81: | train loss: 0.8135483860969543 | train accu: 0.508 | train roc: 1.000 | train f1: 0.45996621621621625\n",
      "batch idx 82: | train loss: 0.8592250943183899 | train accu: 0.480 | train roc: 1.000 | train f1: 0.42753719220246245\n",
      "batch idx 83: | train loss: 0.8104044795036316 | train accu: 0.559 | train roc: 1.000 | train f1: 0.503859526126023\n",
      "batch idx 84: | train loss: 0.7911643981933594 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4932518875838926\n",
      "batch idx 85: | train loss: 0.7901219725608826 | train accu: 0.555 | train roc: 1.000 | train f1: 0.502803657694962\n",
      "batch idx 86: | train loss: 0.7945794463157654 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4931232669238652\n",
      "batch idx 87: | train loss: 0.8420377969741821 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4800722276921888\n",
      "batch idx 88: | train loss: 0.8159220218658447 | train accu: 0.512 | train roc: 1.000 | train f1: 0.46017231466450215\n",
      "batch idx 89: | train loss: 0.8008233904838562 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5225266809152982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx 90: | train loss: 0.8380853533744812 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5059982764876633\n",
      "batch idx 91: | train loss: 0.7919902205467224 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5123141104714912\n",
      "batch idx 92: | train loss: 0.785679042339325 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5241834055522915\n",
      "batch idx 93: | train loss: 0.8284611105918884 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5325112620003001\n",
      "batch idx 94: | train loss: 0.7869013547897339 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5424417325798668\n",
      "batch idx 95: | train loss: 0.7997968196868896 | train accu: 0.547 | train roc: 1.000 | train f1: 0.49164718488213527\n",
      "batch idx 96: | train loss: 0.7961394190788269 | train accu: 0.566 | train roc: 1.000 | train f1: 0.519928600511866\n",
      "batch idx 97: | train loss: 0.8779410719871521 | train accu: 0.504 | train roc: 1.000 | train f1: 0.44780098545777614\n",
      "batch idx 98: | train loss: 0.7865733504295349 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5487046435090437\n",
      "batch idx 99: | train loss: 0.7864004373550415 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5245451440635867\n",
      "batch idx 100: | train loss: 0.8462929129600525 | train accu: 0.516 | train roc: 1.000 | train f1: 0.4625976709417782\n",
      "batch idx 101: | train loss: 0.7949348092079163 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5187089370893709\n",
      "batch idx 102: | train loss: 0.8135164976119995 | train accu: 0.512 | train roc: 1.000 | train f1: 0.46666805309080156\n",
      "batch idx 103: | train loss: 0.8220410943031311 | train accu: 0.547 | train roc: 1.000 | train f1: 0.5073822709975537\n",
      "batch idx 104: | train loss: 0.8179535865783691 | train accu: 0.555 | train roc: 1.000 | train f1: 0.5007945504617781\n",
      "batch idx 105: | train loss: 0.8420991897583008 | train accu: 0.523 | train roc: 1.000 | train f1: 0.476422605957157\n",
      "batch idx 106: | train loss: 0.8447223901748657 | train accu: 0.496 | train roc: 1.000 | train f1: 0.4527330191807955\n",
      "batch idx 107: | train loss: 0.8291905522346497 | train accu: 0.527 | train roc: 1.000 | train f1: 0.480625\n",
      "batch idx 108: | train loss: 0.8332208395004272 | train accu: 0.512 | train roc: 1.000 | train f1: 0.461212588028169\n",
      "batch idx 109: | train loss: 0.8520540595054626 | train accu: 0.520 | train roc: 1.000 | train f1: 0.4573272447610902\n",
      "batch idx 110: | train loss: 0.8127721548080444 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4746675633394384\n",
      "batch idx 111: | train loss: 0.9117959141731262 | train accu: 0.465 | train roc: 1.000 | train f1: 0.38400779376498806\n",
      "batch idx 112: | train loss: 0.8681140542030334 | train accu: 0.480 | train roc: 1.000 | train f1: 0.4202103384309558\n",
      "batch idx 113: | train loss: 0.818686842918396 | train accu: 0.512 | train roc: 1.000 | train f1: 0.46363031914893627\n",
      "batch idx 114: | train loss: 0.7989495396614075 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5309897515867285\n",
      "batch idx 115: | train loss: 0.8182903528213501 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4969767720306514\n",
      "batch idx 116: | train loss: 0.8503780364990234 | train accu: 0.520 | train roc: 1.000 | train f1: 0.45686080470530777\n",
      "batch idx 117: | train loss: 0.8314604163169861 | train accu: 0.520 | train roc: 1.000 | train f1: 0.47597335038106703\n",
      "batch idx 118: | train loss: 0.858186662197113 | train accu: 0.473 | train roc: 1.000 | train f1: 0.41038520587939187\n",
      "batch idx 119: | train loss: 0.8125976324081421 | train accu: 0.559 | train roc: 1.000 | train f1: 0.49555191532258064\n",
      "Epoch: 02 | Epoch Time: 2m 12s\n",
      "\tTrain Loss: 0.830 | Train Acc: 52.64 | Train rocauc: 1.0 | Train f1: 0.46605956230985013%\n",
      "\t Val. Loss: 0.796 |  Val. Acc: 57.13 | Val. rocauc: 1.0 | Val. f1: 0.5103898298379459%\n",
      "batch idx 0: | train loss: 0.8397661447525024 | train accu: 0.500 | train roc: 1.000 | train f1: 0.4356201951876997\n",
      "batch idx 1: | train loss: 0.8777811527252197 | train accu: 0.504 | train roc: 1.000 | train f1: 0.445654800257732\n",
      "batch idx 2: | train loss: 0.8239941000938416 | train accu: 0.547 | train roc: 1.000 | train f1: 0.48660660974444714\n",
      "batch idx 3: | train loss: 0.8071249127388 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5134188122923588\n",
      "batch idx 4: | train loss: 0.8066409230232239 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5147461993750752\n",
      "batch idx 5: | train loss: 0.8361582159996033 | train accu: 0.531 | train roc: 1.000 | train f1: 0.47392011834319525\n",
      "batch idx 6: | train loss: 0.8097310066223145 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4838433476181973\n",
      "batch idx 7: | train loss: 0.8366879820823669 | train accu: 0.496 | train roc: 1.000 | train f1: 0.4292997508065183\n",
      "batch idx 8: | train loss: 0.8525635600090027 | train accu: 0.508 | train roc: 1.000 | train f1: 0.43428473539953616\n",
      "batch idx 9: | train loss: 0.7866712212562561 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5012275139591235\n",
      "batch idx 10: | train loss: 0.7939954400062561 | train accu: 0.555 | train roc: 1.000 | train f1: 0.49200029952076674\n",
      "batch idx 11: | train loss: 0.7964143753051758 | train accu: 0.559 | train roc: 1.000 | train f1: 0.48958228326612907\n",
      "batch idx 12: | train loss: 0.7891783118247986 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5405702227011494\n",
      "batch idx 13: | train loss: 0.798445999622345 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5093186285574554\n",
      "batch idx 14: | train loss: 0.7646734714508057 | train accu: 0.605 | train roc: 1.000 | train f1: 0.5734706586438924\n",
      "batch idx 15: | train loss: 0.7863849997520447 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5381975861583624\n",
      "batch idx 16: | train loss: 0.7703873515129089 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5157543995859213\n",
      "batch idx 17: | train loss: 0.8096770644187927 | train accu: 0.543 | train roc: 1.000 | train f1: 0.47510157239341494\n",
      "batch idx 18: | train loss: 0.8335931301116943 | train accu: 0.512 | train roc: 1.000 | train f1: 0.46880766934433343\n",
      "batch idx 19: | train loss: 0.8263752460479736 | train accu: 0.566 | train roc: 1.000 | train f1: 0.509899966931217\n",
      "batch idx 20: | train loss: 0.8132524490356445 | train accu: 0.543 | train roc: 1.000 | train f1: 0.5029505272952853\n",
      "batch idx 21: | train loss: 0.8422072529792786 | train accu: 0.523 | train roc: 1.000 | train f1: 0.4822538821322121\n",
      "batch idx 22: | train loss: 0.8647037744522095 | train accu: 0.527 | train roc: 1.000 | train f1: 0.48507881748589843\n",
      "batch idx 23: | train loss: 0.8486055135726929 | train accu: 0.500 | train roc: 1.000 | train f1: 0.453553952719878\n",
      "batch idx 24: | train loss: 0.8179134726524353 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4857217261904762\n",
      "batch idx 25: | train loss: 0.7941374182701111 | train accu: 0.590 | train roc: 1.000 | train f1: 0.512643541015851\n",
      "batch idx 26: | train loss: 0.829049825668335 | train accu: 0.512 | train roc: 1.000 | train f1: 0.41566594359460207\n",
      "batch idx 27: | train loss: 0.8104503154754639 | train accu: 0.523 | train roc: 1.000 | train f1: 0.44418460734009235\n",
      "batch idx 28: | train loss: 0.8894472718238831 | train accu: 0.441 | train roc: 1.000 | train f1: 0.33965435606060607\n",
      "batch idx 29: | train loss: 0.8656706213951111 | train accu: 0.496 | train roc: 1.000 | train f1: 0.41565860215053774\n",
      "batch idx 30: | train loss: 0.8137807250022888 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5072731102264482\n",
      "batch idx 31: | train loss: 0.8003641366958618 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5406788793103449\n",
      "batch idx 32: | train loss: 0.8302658200263977 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5140196642872872\n",
      "batch idx 33: | train loss: 0.8185523152351379 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5324815270935961\n",
      "batch idx 34: | train loss: 0.8218068480491638 | train accu: 0.539 | train roc: 1.000 | train f1: 0.5047250574407918\n",
      "batch idx 35: | train loss: 0.8324675559997559 | train accu: 0.543 | train roc: 1.000 | train f1: 0.4947895653768568\n",
      "batch idx 36: | train loss: 0.8059770464897156 | train accu: 0.547 | train roc: 1.000 | train f1: 0.503772598236964\n",
      "batch idx 37: | train loss: 0.7846271395683289 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5409375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx 38: | train loss: 0.7979675531387329 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4828778557626734\n",
      "batch idx 39: | train loss: 0.8481553196907043 | train accu: 0.512 | train roc: 1.000 | train f1: 0.4359012394514768\n",
      "batch idx 40: | train loss: 0.8266497850418091 | train accu: 0.508 | train roc: 1.000 | train f1: 0.4038860452586207\n",
      "batch idx 41: | train loss: 0.8538693785667419 | train accu: 0.508 | train roc: 1.000 | train f1: 0.40945308588594054\n",
      "batch idx 42: | train loss: 0.7946193218231201 | train accu: 0.547 | train roc: 1.000 | train f1: 0.46744701275598755\n",
      "batch idx 43: | train loss: 0.7787404656410217 | train accu: 0.566 | train roc: 1.000 | train f1: 0.48449001547036613\n",
      "batch idx 44: | train loss: 0.8343637585639954 | train accu: 0.520 | train roc: 1.000 | train f1: 0.4450829081632653\n",
      "batch idx 45: | train loss: 0.7988905310630798 | train accu: 0.551 | train roc: 1.000 | train f1: 0.4873813657407407\n",
      "batch idx 46: | train loss: 0.8376437425613403 | train accu: 0.527 | train roc: 1.000 | train f1: 0.45848361100769414\n",
      "batch idx 47: | train loss: 0.8142252564430237 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5176057701919232\n",
      "batch idx 48: | train loss: 0.8070670366287231 | train accu: 0.559 | train roc: 1.000 | train f1: 0.515618239345208\n",
      "batch idx 49: | train loss: 0.807042121887207 | train accu: 0.535 | train roc: 1.000 | train f1: 0.4941899819494585\n",
      "batch idx 50: | train loss: 0.7999750971794128 | train accu: 0.605 | train roc: 1.000 | train f1: 0.5601169575086897\n",
      "batch idx 51: | train loss: 0.8221245408058167 | train accu: 0.535 | train roc: 1.000 | train f1: 0.4870517791970803\n",
      "batch idx 52: | train loss: 0.8286855220794678 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5081129807692308\n",
      "batch idx 53: | train loss: 0.7747201323509216 | train accu: 0.602 | train roc: 1.000 | train f1: 0.553286890327914\n",
      "batch idx 54: | train loss: 0.7862865328788757 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5392602737816438\n",
      "batch idx 55: | train loss: 0.802708089351654 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5318734499862221\n",
      "batch idx 56: | train loss: 0.7980416417121887 | train accu: 0.555 | train roc: 1.000 | train f1: 0.5052862594973426\n",
      "batch idx 57: | train loss: 0.7571316361427307 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5582338948251502\n",
      "batch idx 58: | train loss: 0.8183128833770752 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4823566125499809\n",
      "batch idx 59: | train loss: 0.814243495464325 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5214779091260207\n",
      "batch idx 60: | train loss: 0.8286325931549072 | train accu: 0.523 | train roc: 1.000 | train f1: 0.45355040901711\n",
      "batch idx 61: | train loss: 0.8277539014816284 | train accu: 0.551 | train roc: 1.000 | train f1: 0.5002983272676371\n",
      "batch idx 62: | train loss: 0.7663667798042297 | train accu: 0.578 | train roc: 1.000 | train f1: 0.528484467793513\n",
      "batch idx 63: | train loss: 0.904341459274292 | train accu: 0.457 | train roc: 1.000 | train f1: 0.4058647293519201\n",
      "batch idx 64: | train loss: 0.8040957450866699 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5156320547069004\n",
      "batch idx 65: | train loss: 0.8573164939880371 | train accu: 0.527 | train roc: 1.000 | train f1: 0.4677830599615477\n",
      "batch idx 66: | train loss: 0.8377841114997864 | train accu: 0.516 | train roc: 1.000 | train f1: 0.4533028972914669\n",
      "batch idx 67: | train loss: 0.8063286542892456 | train accu: 0.535 | train roc: 1.000 | train f1: 0.47750685117519254\n",
      "batch idx 68: | train loss: 0.8376126885414124 | train accu: 0.500 | train roc: 1.000 | train f1: 0.4303291471260221\n",
      "batch idx 69: | train loss: 0.8320320248603821 | train accu: 0.520 | train roc: 1.000 | train f1: 0.45364040798611105\n",
      "batch idx 70: | train loss: 0.8109588027000427 | train accu: 0.555 | train roc: 1.000 | train f1: 0.48486392770831144\n",
      "batch idx 71: | train loss: 0.8482136130332947 | train accu: 0.492 | train roc: 1.000 | train f1: 0.4394050817488748\n",
      "batch idx 72: | train loss: 0.8144711256027222 | train accu: 0.566 | train roc: 1.000 | train f1: 0.49956597222222215\n",
      "batch idx 73: | train loss: 0.8310475945472717 | train accu: 0.512 | train roc: 1.000 | train f1: 0.4548512414383562\n",
      "batch idx 74: | train loss: 0.8290862441062927 | train accu: 0.574 | train roc: 1.000 | train f1: 0.507882452293169\n",
      "batch idx 75: | train loss: 0.8007316589355469 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5167502432958477\n",
      "batch idx 76: | train loss: 0.8604543805122375 | train accu: 0.465 | train roc: 1.000 | train f1: 0.40036739534428795\n",
      "batch idx 77: | train loss: 0.8130211234092712 | train accu: 0.535 | train roc: 1.000 | train f1: 0.4742030759898207\n",
      "batch idx 78: | train loss: 0.8115586638450623 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5049412610732105\n",
      "batch idx 79: | train loss: 0.8524567484855652 | train accu: 0.527 | train roc: 1.000 | train f1: 0.4693380376344086\n",
      "batch idx 80: | train loss: 0.830137312412262 | train accu: 0.531 | train roc: 1.000 | train f1: 0.46883769792035923\n",
      "batch idx 81: | train loss: 0.8422185778617859 | train accu: 0.527 | train roc: 1.000 | train f1: 0.4708951372887864\n",
      "batch idx 82: | train loss: 0.8298078179359436 | train accu: 0.535 | train roc: 1.000 | train f1: 0.49387463960992006\n",
      "batch idx 83: | train loss: 0.7902886867523193 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5221037774372339\n",
      "batch idx 84: | train loss: 0.8242576122283936 | train accu: 0.527 | train roc: 1.000 | train f1: 0.48076296250548967\n",
      "batch idx 85: | train loss: 0.8228037357330322 | train accu: 0.527 | train roc: 1.000 | train f1: 0.4781101005626393\n",
      "batch idx 86: | train loss: 0.8376740217208862 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5011344693736246\n",
      "batch idx 87: | train loss: 0.7576806545257568 | train accu: 0.621 | train roc: 1.000 | train f1: 0.5739651286581497\n",
      "batch idx 88: | train loss: 0.7965843081474304 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5339588903743315\n",
      "batch idx 89: | train loss: 0.876865029335022 | train accu: 0.484 | train roc: 1.000 | train f1: 0.42721514302070784\n",
      "batch idx 90: | train loss: 0.813124418258667 | train accu: 0.543 | train roc: 1.000 | train f1: 0.49580838546834227\n",
      "batch idx 91: | train loss: 0.7977404594421387 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5070809659090909\n",
      "batch idx 92: | train loss: 0.761283278465271 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5308745709382151\n",
      "batch idx 93: | train loss: 0.7958876490592957 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4801554273271359\n",
      "batch idx 94: | train loss: 0.772158682346344 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5190694022005389\n",
      "batch idx 95: | train loss: 0.8017226457595825 | train accu: 0.566 | train roc: 1.000 | train f1: 0.49594554539295393\n",
      "batch idx 96: | train loss: 0.8123541474342346 | train accu: 0.527 | train roc: 1.000 | train f1: 0.46150766692546585\n",
      "batch idx 97: | train loss: 0.8067381978034973 | train accu: 0.559 | train roc: 1.000 | train f1: 0.4926488833746898\n",
      "batch idx 98: | train loss: 0.7742548584938049 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4981949010827051\n",
      "batch idx 99: | train loss: 0.8014526963233948 | train accu: 0.535 | train roc: 1.000 | train f1: 0.49419037830472023\n",
      "batch idx 100: | train loss: 0.8006995320320129 | train accu: 0.566 | train roc: 1.000 | train f1: 0.510068294110927\n",
      "batch idx 101: | train loss: 0.7981739044189453 | train accu: 0.602 | train roc: 1.000 | train f1: 0.5547451620162016\n",
      "batch idx 102: | train loss: 0.8037732243537903 | train accu: 0.543 | train roc: 1.000 | train f1: 0.4799116290983607\n",
      "batch idx 103: | train loss: 0.7759624123573303 | train accu: 0.555 | train roc: 1.000 | train f1: 0.5006889664239482\n",
      "batch idx 104: | train loss: 0.72910475730896 | train accu: 0.617 | train roc: 1.000 | train f1: 0.5587523932107216\n",
      "batch idx 105: | train loss: 0.7906432151794434 | train accu: 0.527 | train roc: 1.000 | train f1: 0.44780988670083627\n",
      "batch idx 106: | train loss: 0.8316909074783325 | train accu: 0.512 | train roc: 1.000 | train f1: 0.4377018229166667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx 107: | train loss: 0.7955224514007568 | train accu: 0.547 | train roc: 1.000 | train f1: 0.46089086416742153\n",
      "batch idx 108: | train loss: 0.8513622283935547 | train accu: 0.488 | train roc: 1.000 | train f1: 0.4208551912499496\n",
      "batch idx 109: | train loss: 0.8690065741539001 | train accu: 0.465 | train roc: 1.000 | train f1: 0.4027082717327025\n",
      "batch idx 110: | train loss: 0.8313997983932495 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4941707138216129\n",
      "batch idx 111: | train loss: 0.857966959476471 | train accu: 0.555 | train roc: 1.000 | train f1: 0.5025120070778564\n",
      "batch idx 112: | train loss: 0.8254427313804626 | train accu: 0.520 | train roc: 1.000 | train f1: 0.47967995061816127\n",
      "batch idx 113: | train loss: 0.7855104804039001 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5438577410408396\n",
      "batch idx 114: | train loss: 0.8036777973175049 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4939833263422819\n",
      "batch idx 115: | train loss: 0.79694002866745 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5127949485886119\n",
      "batch idx 116: | train loss: 0.8101292848587036 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5092955992686463\n",
      "batch idx 117: | train loss: 0.8074039220809937 | train accu: 0.547 | train roc: 1.000 | train f1: 0.47676841854557006\n",
      "batch idx 118: | train loss: 0.7813001275062561 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5334526750052554\n",
      "batch idx 119: | train loss: 0.8257695436477661 | train accu: 0.531 | train roc: 1.000 | train f1: 0.45006181815194535\n",
      "Epoch: 03 | Epoch Time: 2m 13s\n",
      "\tTrain Loss: 0.815 | Train Acc: 54.66 | Train rocauc: 1.0 | Train f1: 0.4874370680065535%\n",
      "\t Val. Loss: 0.787 |  Val. Acc: 56.11 | Val. rocauc: 1.0 | Val. f1: 0.4860169523965224%\n",
      "batch idx 0: | train loss: 0.8221502900123596 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4609779784185595\n",
      "batch idx 1: | train loss: 0.763154923915863 | train accu: 0.598 | train roc: 1.000 | train f1: 0.541240694789082\n",
      "batch idx 2: | train loss: 0.758367121219635 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5223801527269674\n",
      "batch idx 3: | train loss: 0.8048219680786133 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5202587261164826\n",
      "batch idx 4: | train loss: 0.8280403017997742 | train accu: 0.551 | train roc: 1.000 | train f1: 0.48885170503696135\n",
      "batch idx 5: | train loss: 0.7554011940956116 | train accu: 0.625 | train roc: 1.000 | train f1: 0.5854380028051233\n",
      "batch idx 6: | train loss: 0.8219908475875854 | train accu: 0.539 | train roc: 1.000 | train f1: 0.49008642431494\n",
      "batch idx 7: | train loss: 0.7888926267623901 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5258456783448673\n",
      "batch idx 8: | train loss: 0.8224276304244995 | train accu: 0.535 | train roc: 1.000 | train f1: 0.4777541866600393\n",
      "batch idx 9: | train loss: 0.7953975200653076 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5457874019824397\n",
      "batch idx 10: | train loss: 0.808825671672821 | train accu: 0.535 | train roc: 1.000 | train f1: 0.46613513660692063\n",
      "batch idx 11: | train loss: 0.7502530217170715 | train accu: 0.617 | train roc: 1.000 | train f1: 0.5674520426492262\n",
      "batch idx 12: | train loss: 0.8071545958518982 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5159934820748557\n",
      "batch idx 13: | train loss: 0.8604316115379333 | train accu: 0.508 | train roc: 1.000 | train f1: 0.46904756637168143\n",
      "batch idx 14: | train loss: 0.8100374937057495 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5121036945657272\n",
      "batch idx 15: | train loss: 0.8207931518554688 | train accu: 0.527 | train roc: 1.000 | train f1: 0.49334455556906615\n",
      "batch idx 16: | train loss: 0.789306640625 | train accu: 0.551 | train roc: 1.000 | train f1: 0.4993546037296038\n",
      "batch idx 17: | train loss: 0.8101938366889954 | train accu: 0.555 | train roc: 1.000 | train f1: 0.49683768866174627\n",
      "batch idx 18: | train loss: 0.802762508392334 | train accu: 0.551 | train roc: 1.000 | train f1: 0.48704121194012306\n",
      "batch idx 19: | train loss: 0.7819482684135437 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5041452216633524\n",
      "batch idx 20: | train loss: 0.849958598613739 | train accu: 0.512 | train roc: 1.000 | train f1: 0.4469720487156332\n",
      "batch idx 21: | train loss: 0.7778764963150024 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5491211407192275\n",
      "batch idx 22: | train loss: 0.8336547017097473 | train accu: 0.520 | train roc: 1.000 | train f1: 0.4613849305394319\n",
      "batch idx 23: | train loss: 0.7988881468772888 | train accu: 0.547 | train roc: 1.000 | train f1: 0.49187288494077824\n",
      "batch idx 24: | train loss: 0.8382735848426819 | train accu: 0.523 | train roc: 1.000 | train f1: 0.45924610550539924\n",
      "batch idx 25: | train loss: 0.8301891684532166 | train accu: 0.555 | train roc: 1.000 | train f1: 0.5059594002695418\n",
      "batch idx 26: | train loss: 0.8079734444618225 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5231831395348838\n",
      "batch idx 27: | train loss: 0.844994306564331 | train accu: 0.539 | train roc: 1.000 | train f1: 0.48475228261898007\n",
      "batch idx 28: | train loss: 0.7870713472366333 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5535628899891482\n",
      "batch idx 29: | train loss: 0.7799664735794067 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5263066520467836\n",
      "batch idx 30: | train loss: 0.8053616285324097 | train accu: 0.539 | train roc: 1.000 | train f1: 0.477884684105203\n",
      "batch idx 31: | train loss: 0.8326372504234314 | train accu: 0.523 | train roc: 1.000 | train f1: 0.457809139379686\n",
      "batch idx 32: | train loss: 0.8367040753364563 | train accu: 0.547 | train roc: 1.000 | train f1: 0.47695872219079943\n",
      "batch idx 33: | train loss: 0.8036205768585205 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5190983527705423\n",
      "batch idx 34: | train loss: 0.8172429800033569 | train accu: 0.551 | train roc: 1.000 | train f1: 0.47948288690476193\n",
      "batch idx 35: | train loss: 0.8050549626350403 | train accu: 0.551 | train roc: 1.000 | train f1: 0.48146487669176025\n",
      "batch idx 36: | train loss: 0.8019184470176697 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4952133694284687\n",
      "batch idx 37: | train loss: 0.7867511510848999 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5310566468523991\n",
      "batch idx 38: | train loss: 0.7823196053504944 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5198306693747792\n",
      "batch idx 39: | train loss: 0.8114405870437622 | train accu: 0.551 | train roc: 1.000 | train f1: 0.5024720292135457\n",
      "batch idx 40: | train loss: 0.7959292531013489 | train accu: 0.574 | train roc: 1.000 | train f1: 0.517299237912045\n",
      "batch idx 41: | train loss: 0.8096226453781128 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5092389612176331\n",
      "batch idx 42: | train loss: 0.7926567196846008 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4990714343077417\n",
      "batch idx 43: | train loss: 0.8440567851066589 | train accu: 0.516 | train roc: 1.000 | train f1: 0.45250097299562897\n",
      "batch idx 44: | train loss: 0.776688277721405 | train accu: 0.617 | train roc: 1.000 | train f1: 0.5730495104492349\n",
      "batch idx 45: | train loss: 0.7893598079681396 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5140581162910315\n",
      "batch idx 46: | train loss: 0.8272777199745178 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4794356003377178\n",
      "batch idx 47: | train loss: 0.7275776863098145 | train accu: 0.641 | train roc: 1.000 | train f1: 0.5901215284258464\n",
      "batch idx 48: | train loss: 0.8705751895904541 | train accu: 0.496 | train roc: 1.000 | train f1: 0.4335881500876074\n",
      "batch idx 49: | train loss: 0.8246846199035645 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4881721085120208\n",
      "batch idx 50: | train loss: 0.8143306970596313 | train accu: 0.559 | train roc: 1.000 | train f1: 0.4991159539473684\n",
      "batch idx 51: | train loss: 0.8440867066383362 | train accu: 0.531 | train roc: 1.000 | train f1: 0.478063973063973\n",
      "batch idx 52: | train loss: 0.7683749198913574 | train accu: 0.605 | train roc: 1.000 | train f1: 0.5467314697497623\n",
      "batch idx 53: | train loss: 0.7564936280250549 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5353511100386101\n",
      "batch idx 54: | train loss: 0.8086898326873779 | train accu: 0.551 | train roc: 1.000 | train f1: 0.48480255606793154\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx 55: | train loss: 0.7520896792411804 | train accu: 0.602 | train roc: 1.000 | train f1: 0.5429211696919034\n",
      "batch idx 56: | train loss: 0.8168467283248901 | train accu: 0.531 | train roc: 1.000 | train f1: 0.4848153918214402\n",
      "batch idx 57: | train loss: 0.819250762462616 | train accu: 0.539 | train roc: 1.000 | train f1: 0.48258404563608026\n",
      "batch idx 58: | train loss: 0.8042557239532471 | train accu: 0.562 | train roc: 1.000 | train f1: 0.49131988396624476\n",
      "batch idx 59: | train loss: 0.768286406993866 | train accu: 0.629 | train roc: 1.000 | train f1: 0.5794335332817337\n",
      "batch idx 60: | train loss: 0.7985471487045288 | train accu: 0.551 | train roc: 1.000 | train f1: 0.508794452109667\n",
      "batch idx 61: | train loss: 0.8237888216972351 | train accu: 0.543 | train roc: 1.000 | train f1: 0.49226184039792387\n",
      "batch idx 62: | train loss: 0.788175642490387 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5399130147202443\n",
      "batch idx 63: | train loss: 0.8006595969200134 | train accu: 0.535 | train roc: 1.000 | train f1: 0.4856283360515976\n",
      "batch idx 64: | train loss: 0.8546941876411438 | train accu: 0.516 | train roc: 1.000 | train f1: 0.4554326265898015\n",
      "batch idx 65: | train loss: 0.7883718013763428 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5099427913688841\n",
      "batch idx 66: | train loss: 0.7999706864356995 | train accu: 0.582 | train roc: 1.000 | train f1: 0.521754645187522\n",
      "batch idx 67: | train loss: 0.7764458656311035 | train accu: 0.551 | train roc: 1.000 | train f1: 0.471750322107695\n",
      "batch idx 68: | train loss: 0.8026143908500671 | train accu: 0.543 | train roc: 1.000 | train f1: 0.46356691335617983\n",
      "batch idx 69: | train loss: 0.8531656265258789 | train accu: 0.520 | train roc: 1.000 | train f1: 0.44177827380952384\n",
      "batch idx 70: | train loss: 0.7809300422668457 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5359515806250599\n",
      "batch idx 71: | train loss: 0.7946310639381409 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5438333068775767\n",
      "batch idx 72: | train loss: 0.7860361933708191 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5435836226851852\n",
      "batch idx 73: | train loss: 0.8124551773071289 | train accu: 0.543 | train roc: 1.000 | train f1: 0.48323231680721146\n",
      "batch idx 74: | train loss: 0.8126697540283203 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5231417213528521\n",
      "batch idx 75: | train loss: 0.8502234220504761 | train accu: 0.531 | train roc: 1.000 | train f1: 0.4787690839694657\n",
      "batch idx 76: | train loss: 0.8233013153076172 | train accu: 0.551 | train roc: 1.000 | train f1: 0.49977442872463285\n",
      "batch idx 77: | train loss: 0.8108360171318054 | train accu: 0.551 | train roc: 1.000 | train f1: 0.5064867424242424\n",
      "batch idx 78: | train loss: 0.8176917433738708 | train accu: 0.543 | train roc: 1.000 | train f1: 0.4884999702522608\n",
      "batch idx 79: | train loss: 0.8144638538360596 | train accu: 0.531 | train roc: 1.000 | train f1: 0.46488095238095234\n",
      "batch idx 80: | train loss: 0.8303816914558411 | train accu: 0.504 | train roc: 1.000 | train f1: 0.4368303571428571\n",
      "batch idx 81: | train loss: 0.7490390539169312 | train accu: 0.617 | train roc: 1.000 | train f1: 0.554135101010101\n",
      "batch idx 82: | train loss: 0.8653892874717712 | train accu: 0.488 | train roc: 1.000 | train f1: 0.4056841176503515\n",
      "batch idx 83: | train loss: 0.7874513864517212 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5058173076923077\n",
      "batch idx 84: | train loss: 0.8795100450515747 | train accu: 0.484 | train roc: 1.000 | train f1: 0.40225220182116733\n",
      "batch idx 85: | train loss: 0.7972448468208313 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4851670269392243\n",
      "batch idx 86: | train loss: 0.790232241153717 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5422564021589343\n",
      "batch idx 87: | train loss: 0.854462742805481 | train accu: 0.504 | train roc: 1.000 | train f1: 0.44962288123322847\n",
      "batch idx 88: | train loss: 0.812965989112854 | train accu: 0.559 | train roc: 1.000 | train f1: 0.504390133171913\n",
      "batch idx 89: | train loss: 0.8057895302772522 | train accu: 0.555 | train roc: 1.000 | train f1: 0.5038107638888889\n",
      "batch idx 90: | train loss: 0.8125763535499573 | train accu: 0.551 | train roc: 1.000 | train f1: 0.5020420427786499\n",
      "batch idx 91: | train loss: 0.8002973198890686 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5233799390968509\n",
      "batch idx 92: | train loss: 0.785569965839386 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5312827015871171\n",
      "batch idx 93: | train loss: 0.7655701041221619 | train accu: 0.602 | train roc: 1.000 | train f1: 0.5365125733531697\n",
      "batch idx 94: | train loss: 0.7692452669143677 | train accu: 0.594 | train roc: 1.000 | train f1: 0.513882137740483\n",
      "batch idx 95: | train loss: 0.7851619720458984 | train accu: 0.570 | train roc: 1.000 | train f1: 0.4867437067672539\n",
      "batch idx 96: | train loss: 0.842239499092102 | train accu: 0.527 | train roc: 1.000 | train f1: 0.45431528589696746\n",
      "batch idx 97: | train loss: 0.7697270512580872 | train accu: 0.555 | train roc: 1.000 | train f1: 0.48504704301075263\n",
      "batch idx 98: | train loss: 0.8212467432022095 | train accu: 0.504 | train roc: 1.000 | train f1: 0.44089894480519476\n",
      "batch idx 99: | train loss: 0.8047252297401428 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4953765325634584\n",
      "batch idx 100: | train loss: 0.8189408183097839 | train accu: 0.547 | train roc: 1.000 | train f1: 0.5022401693704027\n",
      "batch idx 101: | train loss: 0.8421860933303833 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5273303000491883\n",
      "batch idx 102: | train loss: 0.860000729560852 | train accu: 0.512 | train roc: 1.000 | train f1: 0.46410542326067483\n",
      "batch idx 103: | train loss: 0.8271075487136841 | train accu: 0.543 | train roc: 1.000 | train f1: 0.49296699700822744\n",
      "batch idx 104: | train loss: 0.8287580609321594 | train accu: 0.523 | train roc: 1.000 | train f1: 0.46619533534708324\n",
      "batch idx 105: | train loss: 0.8162782192230225 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5281751007613077\n",
      "batch idx 106: | train loss: 0.7996863126754761 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5054441213819237\n",
      "batch idx 107: | train loss: 0.8242225050926208 | train accu: 0.535 | train roc: 1.000 | train f1: 0.4645460944526204\n",
      "batch idx 108: | train loss: 0.8204866051673889 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4629525862068965\n",
      "batch idx 109: | train loss: 0.817682683467865 | train accu: 0.531 | train roc: 1.000 | train f1: 0.4356533989902582\n",
      "batch idx 110: | train loss: 0.7528842687606812 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5020856507230256\n",
      "batch idx 111: | train loss: 0.7935155630111694 | train accu: 0.559 | train roc: 1.000 | train f1: 0.47562961647727275\n",
      "batch idx 112: | train loss: 0.7851653099060059 | train accu: 0.559 | train roc: 1.000 | train f1: 0.47828556931114463\n",
      "batch idx 113: | train loss: 0.7799898982048035 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5316549877506486\n",
      "batch idx 114: | train loss: 0.771083652973175 | train accu: 0.574 | train roc: 1.000 | train f1: 0.49772481806789204\n",
      "batch idx 115: | train loss: 0.833702564239502 | train accu: 0.527 | train roc: 1.000 | train f1: 0.45780222039473684\n",
      "batch idx 116: | train loss: 0.8565746545791626 | train accu: 0.492 | train roc: 1.000 | train f1: 0.4311706016299137\n",
      "batch idx 117: | train loss: 0.8084431886672974 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5095555342535788\n",
      "batch idx 118: | train loss: 0.8722732663154602 | train accu: 0.516 | train roc: 1.000 | train f1: 0.4569992728778628\n",
      "batch idx 119: | train loss: 0.7742720246315002 | train accu: 0.605 | train roc: 1.000 | train f1: 0.5509433243071818\n",
      "Epoch: 04 | Epoch Time: 2m 14s\n",
      "\tTrain Loss: 0.807 | Train Acc: 55.68 | Train rocauc: 1.0 | Train f1: 0.49796057482791906%\n",
      "\t Val. Loss: 0.778 |  Val. Acc: 58.98 | Val. rocauc: 1.0 | Val. f1: 0.5361448298894803%\n",
      "batch idx 0: | train loss: 0.7891891598701477 | train accu: 0.543 | train roc: 1.000 | train f1: 0.4932107506717961\n",
      "batch idx 1: | train loss: 0.7908315062522888 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5129654255319149\n",
      "batch idx 2: | train loss: 0.8124152421951294 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4839899370208105\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx 3: | train loss: 0.7630407810211182 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5261315049446988\n",
      "batch idx 4: | train loss: 0.7705289125442505 | train accu: 0.609 | train roc: 1.000 | train f1: 0.5403501578263291\n",
      "batch idx 5: | train loss: 0.7629514336585999 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5313426948956841\n",
      "batch idx 6: | train loss: 0.7951694130897522 | train accu: 0.562 | train roc: 1.000 | train f1: 0.49124370594159106\n",
      "batch idx 7: | train loss: 0.8275613784790039 | train accu: 0.531 | train roc: 1.000 | train f1: 0.4653404209621993\n",
      "batch idx 8: | train loss: 0.8125942349433899 | train accu: 0.531 | train roc: 1.000 | train f1: 0.4794882015306123\n",
      "batch idx 9: | train loss: 0.7771074771881104 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5492039295392954\n",
      "batch idx 10: | train loss: 0.805815577507019 | train accu: 0.566 | train roc: 1.000 | train f1: 0.508820564516129\n",
      "batch idx 11: | train loss: 0.8025102019309998 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5331022168760221\n",
      "batch idx 12: | train loss: 0.860244631767273 | train accu: 0.500 | train roc: 1.000 | train f1: 0.4556699292796734\n",
      "batch idx 13: | train loss: 0.7731088995933533 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5295738064423353\n",
      "batch idx 14: | train loss: 0.8512151837348938 | train accu: 0.500 | train roc: 1.000 | train f1: 0.4421457607433218\n",
      "batch idx 15: | train loss: 0.8240463137626648 | train accu: 0.527 | train roc: 1.000 | train f1: 0.47194273724557223\n",
      "batch idx 16: | train loss: 0.7672537565231323 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5188478116710875\n",
      "batch idx 17: | train loss: 0.8812963962554932 | train accu: 0.488 | train roc: 1.000 | train f1: 0.4261967035136202\n",
      "batch idx 18: | train loss: 0.7912498116493225 | train accu: 0.547 | train roc: 1.000 | train f1: 0.47895226370213106\n",
      "batch idx 19: | train loss: 0.8330740928649902 | train accu: 0.520 | train roc: 1.000 | train f1: 0.43626434515392143\n",
      "batch idx 20: | train loss: 0.8189504146575928 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4802898095081021\n",
      "batch idx 21: | train loss: 0.7880197167396545 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5136564334637965\n",
      "batch idx 22: | train loss: 0.7884895205497742 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5241767497464136\n",
      "batch idx 23: | train loss: 0.795752227306366 | train accu: 0.605 | train roc: 1.000 | train f1: 0.550254127258909\n",
      "batch idx 24: | train loss: 0.7717652320861816 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5571419864160454\n",
      "batch idx 25: | train loss: 0.8139280676841736 | train accu: 0.547 | train roc: 1.000 | train f1: 0.5018646042330981\n",
      "batch idx 26: | train loss: 0.8419368863105774 | train accu: 0.543 | train roc: 1.000 | train f1: 0.49562581712698156\n",
      "batch idx 27: | train loss: 0.7879749536514282 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5071391782407407\n",
      "batch idx 28: | train loss: 0.7945038676261902 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4978374756335283\n",
      "batch idx 29: | train loss: 0.7895317077636719 | train accu: 0.613 | train roc: 1.000 | train f1: 0.5620413115530303\n",
      "batch idx 30: | train loss: 0.8292271494865417 | train accu: 0.520 | train roc: 1.000 | train f1: 0.4718053143209323\n",
      "batch idx 31: | train loss: 0.7741106152534485 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5039265487242366\n",
      "batch idx 32: | train loss: 0.7692030668258667 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5014586426868821\n",
      "batch idx 33: | train loss: 0.7841423749923706 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5107655502392345\n",
      "batch idx 34: | train loss: 0.763809323310852 | train accu: 0.602 | train roc: 1.000 | train f1: 0.537262104263177\n",
      "batch idx 35: | train loss: 0.82901531457901 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4852161681966243\n",
      "batch idx 36: | train loss: 0.7965936064720154 | train accu: 0.555 | train roc: 1.000 | train f1: 0.49628368374296716\n",
      "batch idx 37: | train loss: 0.7855422496795654 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5101399529372628\n",
      "batch idx 38: | train loss: 0.8346589803695679 | train accu: 0.523 | train roc: 1.000 | train f1: 0.4650604551920342\n",
      "batch idx 39: | train loss: 0.8037302494049072 | train accu: 0.523 | train roc: 1.000 | train f1: 0.4719856908956739\n",
      "batch idx 40: | train loss: 0.7958275675773621 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5078145099048108\n",
      "batch idx 41: | train loss: 0.7810154557228088 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4685600906449501\n",
      "batch idx 42: | train loss: 0.7972896695137024 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5199546242915771\n",
      "batch idx 43: | train loss: 0.8019059300422668 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5219695005186309\n",
      "batch idx 44: | train loss: 0.7894673347473145 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5143453663793103\n",
      "batch idx 45: | train loss: 0.8230664730072021 | train accu: 0.523 | train roc: 1.000 | train f1: 0.4897682081828285\n",
      "batch idx 46: | train loss: 0.826721727848053 | train accu: 0.551 | train roc: 1.000 | train f1: 0.499016419657867\n",
      "batch idx 47: | train loss: 0.7829776406288147 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5244903354881243\n",
      "batch idx 48: | train loss: 0.8237670660018921 | train accu: 0.535 | train roc: 1.000 | train f1: 0.4906030868379134\n",
      "batch idx 49: | train loss: 0.820686936378479 | train accu: 0.562 | train roc: 1.000 | train f1: 0.4985822858262495\n",
      "batch idx 50: | train loss: 0.7728310227394104 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5488915636677206\n",
      "batch idx 51: | train loss: 0.8251898288726807 | train accu: 0.520 | train roc: 1.000 | train f1: 0.45234015529141114\n",
      "batch idx 52: | train loss: 0.8343368768692017 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4703810724251649\n",
      "batch idx 53: | train loss: 0.7803069353103638 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5058415032679738\n",
      "batch idx 54: | train loss: 0.8215520977973938 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4925026633522728\n",
      "batch idx 55: | train loss: 0.8142184615135193 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5042047208644199\n",
      "batch idx 56: | train loss: 0.828131377696991 | train accu: 0.559 | train roc: 1.000 | train f1: 0.48912287205948074\n",
      "batch idx 57: | train loss: 0.7839450836181641 | train accu: 0.555 | train roc: 1.000 | train f1: 0.49433801233261426\n",
      "batch idx 58: | train loss: 0.8317330479621887 | train accu: 0.535 | train roc: 1.000 | train f1: 0.4680735930735931\n",
      "batch idx 59: | train loss: 0.7652378678321838 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5471774974186887\n",
      "batch idx 60: | train loss: 0.8137596845626831 | train accu: 0.535 | train roc: 1.000 | train f1: 0.4774068961466166\n",
      "batch idx 61: | train loss: 0.7884551286697388 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5276436369337185\n",
      "batch idx 62: | train loss: 0.7687376141548157 | train accu: 0.609 | train roc: 1.000 | train f1: 0.5616010998307952\n",
      "batch idx 63: | train loss: 0.8086491823196411 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5252393557631471\n",
      "batch idx 64: | train loss: 0.7877271175384521 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5047330883534943\n",
      "batch idx 65: | train loss: 0.7382106184959412 | train accu: 0.633 | train roc: 1.000 | train f1: 0.5791583606941838\n",
      "batch idx 66: | train loss: 0.7646945714950562 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5490439682454059\n",
      "batch idx 67: | train loss: 0.7928034663200378 | train accu: 0.570 | train roc: 1.000 | train f1: 0.503712150621118\n",
      "batch idx 68: | train loss: 0.765867292881012 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5396582645440252\n",
      "batch idx 69: | train loss: 0.7984444499015808 | train accu: 0.547 | train roc: 1.000 | train f1: 0.49151329014409684\n",
      "batch idx 70: | train loss: 0.8490744829177856 | train accu: 0.539 | train roc: 1.000 | train f1: 0.48632610939112486\n",
      "batch idx 71: | train loss: 0.754955530166626 | train accu: 0.621 | train roc: 1.000 | train f1: 0.5671065865167878\n",
      "batch idx 72: | train loss: 0.7435512542724609 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5493698826082667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx 73: | train loss: 0.775240421295166 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5223992145394414\n",
      "batch idx 74: | train loss: 0.7773873209953308 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4905830753353973\n",
      "batch idx 75: | train loss: 0.8148161768913269 | train accu: 0.543 | train roc: 1.000 | train f1: 0.47712779050374743\n",
      "batch idx 76: | train loss: 0.8232009410858154 | train accu: 0.496 | train roc: 1.000 | train f1: 0.42905254633344025\n",
      "batch idx 77: | train loss: 0.8271745443344116 | train accu: 0.512 | train roc: 1.000 | train f1: 0.4399283622007132\n",
      "batch idx 78: | train loss: 0.8067781925201416 | train accu: 0.559 | train roc: 1.000 | train f1: 0.4924547697368421\n",
      "batch idx 79: | train loss: 0.7696845531463623 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5152638762511373\n",
      "batch idx 80: | train loss: 0.8141669631004333 | train accu: 0.566 | train roc: 1.000 | train f1: 0.4992839845092924\n",
      "batch idx 81: | train loss: 0.7854375839233398 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5350846364682604\n",
      "batch idx 82: | train loss: 0.7733391523361206 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5157085553207532\n",
      "batch idx 83: | train loss: 0.7978301644325256 | train accu: 0.520 | train roc: 1.000 | train f1: 0.4606401141961487\n",
      "batch idx 84: | train loss: 0.7464345097541809 | train accu: 0.617 | train roc: 1.000 | train f1: 0.571546811988844\n",
      "batch idx 85: | train loss: 0.7166789770126343 | train accu: 0.648 | train roc: 1.000 | train f1: 0.5972772669625872\n",
      "batch idx 86: | train loss: 0.7968231439590454 | train accu: 0.543 | train roc: 1.000 | train f1: 0.4818948412698413\n",
      "batch idx 87: | train loss: 0.8645253777503967 | train accu: 0.477 | train roc: 1.000 | train f1: 0.41306743801016477\n",
      "batch idx 88: | train loss: 0.7612563371658325 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5082726957726957\n",
      "batch idx 89: | train loss: 0.8011007905006409 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4857193587662338\n",
      "batch idx 90: | train loss: 0.8258200883865356 | train accu: 0.523 | train roc: 1.000 | train f1: 0.4672286428667318\n",
      "batch idx 91: | train loss: 0.8197504878044128 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5299032245651839\n",
      "batch idx 92: | train loss: 0.7892647981643677 | train accu: 0.609 | train roc: 1.000 | train f1: 0.5445245327102803\n",
      "batch idx 93: | train loss: 0.7875919342041016 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5352493248369673\n",
      "batch idx 94: | train loss: 0.7749553918838501 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5379007089692859\n",
      "batch idx 95: | train loss: 0.8355714082717896 | train accu: 0.523 | train roc: 1.000 | train f1: 0.4690516973834989\n",
      "batch idx 96: | train loss: 0.8182271122932434 | train accu: 0.547 | train roc: 1.000 | train f1: 0.47776009043425255\n",
      "batch idx 97: | train loss: 0.8069867491722107 | train accu: 0.551 | train roc: 1.000 | train f1: 0.4870490916955017\n",
      "batch idx 98: | train loss: 0.7809950113296509 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5249825590064757\n",
      "batch idx 99: | train loss: 0.8378757834434509 | train accu: 0.535 | train roc: 1.000 | train f1: 0.4770721477717641\n",
      "batch idx 100: | train loss: 0.7534412145614624 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5220243298368298\n",
      "batch idx 101: | train loss: 0.8165050745010376 | train accu: 0.523 | train roc: 1.000 | train f1: 0.4459111530220269\n",
      "batch idx 102: | train loss: 0.792797327041626 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4753990058246732\n",
      "batch idx 103: | train loss: 0.8386971354484558 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4729959273372447\n",
      "batch idx 104: | train loss: 0.7720227241516113 | train accu: 0.602 | train roc: 1.000 | train f1: 0.5607975652380778\n",
      "batch idx 105: | train loss: 0.7999856472015381 | train accu: 0.551 | train roc: 1.000 | train f1: 0.48593800042703106\n",
      "batch idx 106: | train loss: 0.8102449774742126 | train accu: 0.543 | train roc: 1.000 | train f1: 0.48159816777172626\n",
      "batch idx 107: | train loss: 0.8429369330406189 | train accu: 0.477 | train roc: 1.000 | train f1: 0.41253944600280507\n",
      "batch idx 108: | train loss: 0.778350293636322 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5049001675106379\n",
      "batch idx 109: | train loss: 0.7986442446708679 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5041347646918299\n",
      "batch idx 110: | train loss: 0.8025076985359192 | train accu: 0.574 | train roc: 1.000 | train f1: 0.51142578125\n",
      "batch idx 111: | train loss: 0.8092305064201355 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5005546389751552\n",
      "batch idx 112: | train loss: 0.7644588947296143 | train accu: 0.605 | train roc: 1.000 | train f1: 0.5549346135283636\n",
      "batch idx 113: | train loss: 0.8072147369384766 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5122600655430711\n",
      "batch idx 114: | train loss: 0.7989253401756287 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5234299897861092\n",
      "batch idx 115: | train loss: 0.8486343026161194 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4738717082467082\n",
      "batch idx 116: | train loss: 0.7621521353721619 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5162411305581835\n",
      "batch idx 117: | train loss: 0.7474846243858337 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5239627330854874\n",
      "batch idx 118: | train loss: 0.8043962121009827 | train accu: 0.543 | train roc: 1.000 | train f1: 0.4846880893446488\n",
      "batch idx 119: | train loss: 0.7929567694664001 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5052805683973447\n",
      "Epoch: 05 | Epoch Time: 2m 10s\n",
      "\tTrain Loss: 0.798 | Train Acc: 56.12 | Train rocauc: 1.0 | Train f1: 0.5029010037370854%\n",
      "\t Val. Loss: 0.764 |  Val. Acc: 59.19 | Val. rocauc: 1.0 | Val. f1: 0.5332989939405202%\n",
      "batch idx 0: | train loss: 0.804491400718689 | train accu: 0.543 | train roc: 1.000 | train f1: 0.48515102527537374\n",
      "batch idx 1: | train loss: 0.7866907715797424 | train accu: 0.555 | train roc: 1.000 | train f1: 0.5028205749804666\n",
      "batch idx 2: | train loss: 0.7783126831054688 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5247209536280021\n",
      "batch idx 3: | train loss: 0.8041306138038635 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5353497458360372\n",
      "batch idx 4: | train loss: 0.747445821762085 | train accu: 0.645 | train roc: 1.000 | train f1: 0.5906418010752688\n",
      "batch idx 5: | train loss: 0.7622595429420471 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5444456713780919\n",
      "batch idx 6: | train loss: 0.8145206570625305 | train accu: 0.543 | train roc: 1.000 | train f1: 0.49090450879765396\n",
      "batch idx 7: | train loss: 0.7921248078346252 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5415131349334192\n",
      "batch idx 8: | train loss: 0.8081090450286865 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4832699506212169\n",
      "batch idx 9: | train loss: 0.8463417887687683 | train accu: 0.492 | train roc: 1.000 | train f1: 0.42922440549047036\n",
      "batch idx 10: | train loss: 0.7017124891281128 | train accu: 0.652 | train roc: 1.000 | train f1: 0.5983729421815067\n",
      "batch idx 11: | train loss: 0.7584821581840515 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5210097886631717\n",
      "batch idx 12: | train loss: 0.7829306125640869 | train accu: 0.570 | train roc: 1.000 | train f1: 0.49649325578312453\n",
      "batch idx 13: | train loss: 0.7761660218238831 | train accu: 0.562 | train roc: 1.000 | train f1: 0.49401745495495497\n",
      "batch idx 14: | train loss: 0.8126257658004761 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5082259002939735\n",
      "batch idx 15: | train loss: 0.7897235155105591 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5063504607519302\n",
      "batch idx 16: | train loss: 0.8242492079734802 | train accu: 0.516 | train roc: 1.000 | train f1: 0.44985368930432623\n",
      "batch idx 17: | train loss: 0.7771785259246826 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5231394443554362\n",
      "batch idx 18: | train loss: 0.8434481620788574 | train accu: 0.535 | train roc: 1.000 | train f1: 0.4771471088435375\n",
      "batch idx 19: | train loss: 0.8106909394264221 | train accu: 0.559 | train roc: 1.000 | train f1: 0.4953260281385281\n",
      "batch idx 20: | train loss: 0.7747660279273987 | train accu: 0.574 | train roc: 1.000 | train f1: 0.516340960351377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx 21: | train loss: 0.832325279712677 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4967416638069092\n",
      "batch idx 22: | train loss: 0.7989644408226013 | train accu: 0.547 | train roc: 1.000 | train f1: 0.49873820479962727\n",
      "batch idx 23: | train loss: 0.7444006204605103 | train accu: 0.645 | train roc: 1.000 | train f1: 0.6013655851953799\n",
      "batch idx 24: | train loss: 0.7631651163101196 | train accu: 0.598 | train roc: 1.000 | train f1: 0.539682911879409\n",
      "batch idx 25: | train loss: 0.7353612184524536 | train accu: 0.633 | train roc: 1.000 | train f1: 0.5850014400921659\n",
      "batch idx 26: | train loss: 0.7667255401611328 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5507612179487179\n",
      "batch idx 27: | train loss: 0.8502188920974731 | train accu: 0.516 | train roc: 1.000 | train f1: 0.4505359378711695\n",
      "batch idx 28: | train loss: 0.7374197840690613 | train accu: 0.617 | train roc: 1.000 | train f1: 0.5620039682539683\n",
      "batch idx 29: | train loss: 0.796433687210083 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5188478812465243\n",
      "batch idx 30: | train loss: 0.770796537399292 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5435052053857763\n",
      "batch idx 31: | train loss: 0.8170012831687927 | train accu: 0.539 | train roc: 1.000 | train f1: 0.47860314909396884\n",
      "batch idx 32: | train loss: 0.8517550230026245 | train accu: 0.531 | train roc: 1.000 | train f1: 0.46203887000170524\n",
      "batch idx 33: | train loss: 0.7954214811325073 | train accu: 0.555 | train roc: 1.000 | train f1: 0.5038633812770204\n",
      "batch idx 34: | train loss: 0.7527084946632385 | train accu: 0.605 | train roc: 1.000 | train f1: 0.5518168810062183\n",
      "batch idx 35: | train loss: 0.7796847820281982 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5374383960573477\n",
      "batch idx 36: | train loss: 0.8167601227760315 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5028714917339485\n",
      "batch idx 37: | train loss: 0.7743325233459473 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5390150556680162\n",
      "batch idx 38: | train loss: 0.7995204925537109 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5356535719008994\n",
      "batch idx 39: | train loss: 0.784744381904602 | train accu: 0.602 | train roc: 1.000 | train f1: 0.5507112202615854\n",
      "batch idx 40: | train loss: 0.8006080985069275 | train accu: 0.586 | train roc: 1.000 | train f1: 0.517416866617448\n",
      "batch idx 41: | train loss: 0.7612330913543701 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5040926593660968\n",
      "batch idx 42: | train loss: 0.7392410039901733 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5177153044659422\n",
      "batch idx 43: | train loss: 0.8570196628570557 | train accu: 0.496 | train roc: 1.000 | train f1: 0.4202023909395973\n",
      "batch idx 44: | train loss: 0.7776440978050232 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5161868446267467\n",
      "batch idx 45: | train loss: 0.8426568508148193 | train accu: 0.527 | train roc: 1.000 | train f1: 0.4526717661149825\n",
      "batch idx 46: | train loss: 0.8034839630126953 | train accu: 0.566 | train roc: 1.000 | train f1: 0.4996191781028352\n",
      "batch idx 47: | train loss: 0.8046594262123108 | train accu: 0.543 | train roc: 1.000 | train f1: 0.4735243055555555\n",
      "batch idx 48: | train loss: 0.7533839344978333 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5386659858132783\n",
      "batch idx 49: | train loss: 0.8241727948188782 | train accu: 0.535 | train roc: 1.000 | train f1: 0.4806413761570618\n",
      "batch idx 50: | train loss: 0.7686163187026978 | train accu: 0.617 | train roc: 1.000 | train f1: 0.5644578107960742\n",
      "batch idx 51: | train loss: 0.7547881007194519 | train accu: 0.617 | train roc: 1.000 | train f1: 0.5673239300933902\n",
      "batch idx 52: | train loss: 0.8653556704521179 | train accu: 0.492 | train roc: 1.000 | train f1: 0.4323533951382552\n",
      "batch idx 53: | train loss: 0.803911566734314 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5473625725689405\n",
      "batch idx 54: | train loss: 0.8480368852615356 | train accu: 0.527 | train roc: 1.000 | train f1: 0.47964591298433945\n",
      "batch idx 55: | train loss: 0.778486430644989 | train accu: 0.570 | train roc: 1.000 | train f1: 0.524732142857143\n",
      "batch idx 56: | train loss: 0.7654382586479187 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5098833777130225\n",
      "batch idx 57: | train loss: 0.7853876948356628 | train accu: 0.543 | train roc: 1.000 | train f1: 0.4863882899137476\n",
      "batch idx 58: | train loss: 0.8023885488510132 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5148856466876972\n",
      "batch idx 59: | train loss: 0.8428224921226501 | train accu: 0.520 | train roc: 1.000 | train f1: 0.44687237284573356\n",
      "batch idx 60: | train loss: 0.8134188055992126 | train accu: 0.559 | train roc: 1.000 | train f1: 0.4899503049291642\n",
      "batch idx 61: | train loss: 0.7909111380577087 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4720715010141987\n",
      "batch idx 62: | train loss: 0.8027174472808838 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5121868415456883\n",
      "batch idx 63: | train loss: 0.7850707173347473 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5046106631211806\n",
      "batch idx 64: | train loss: 0.7832186222076416 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5269281914893618\n",
      "batch idx 65: | train loss: 0.8727418184280396 | train accu: 0.504 | train roc: 1.000 | train f1: 0.4459535256410257\n",
      "batch idx 66: | train loss: 0.7880098223686218 | train accu: 0.590 | train roc: 1.000 | train f1: 0.533858322409666\n",
      "batch idx 67: | train loss: 0.7692798972129822 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5391879808143353\n",
      "batch idx 68: | train loss: 0.8118701577186584 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5083602398523984\n",
      "batch idx 69: | train loss: 0.7491282224655151 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5449193727758007\n",
      "batch idx 70: | train loss: 0.7689033150672913 | train accu: 0.609 | train roc: 1.000 | train f1: 0.551359649122807\n",
      "batch idx 71: | train loss: 0.7736407518386841 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5098195074156613\n",
      "batch idx 72: | train loss: 0.8048161864280701 | train accu: 0.539 | train roc: 1.000 | train f1: 0.47248252146868336\n",
      "batch idx 73: | train loss: 0.7719473838806152 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5137308980082418\n",
      "batch idx 74: | train loss: 0.7792355418205261 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5201108176389527\n",
      "batch idx 75: | train loss: 0.8040966987609863 | train accu: 0.531 | train roc: 1.000 | train f1: 0.46264564043209877\n",
      "batch idx 76: | train loss: 0.7833448648452759 | train accu: 0.562 | train roc: 1.000 | train f1: 0.49918413765822783\n",
      "batch idx 77: | train loss: 0.7845881581306458 | train accu: 0.559 | train roc: 1.000 | train f1: 0.4982498313090419\n",
      "batch idx 78: | train loss: 0.8197811245918274 | train accu: 0.539 | train roc: 1.000 | train f1: 0.47980062611892893\n",
      "batch idx 79: | train loss: 0.8212178349494934 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5229902862630751\n",
      "batch idx 80: | train loss: 0.8356013298034668 | train accu: 0.520 | train roc: 1.000 | train f1: 0.4708398704195582\n",
      "batch idx 81: | train loss: 0.8115718960762024 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5040709047656495\n",
      "batch idx 82: | train loss: 0.8009423613548279 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5130986987818382\n",
      "batch idx 83: | train loss: 0.7998232841491699 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5118009868421052\n",
      "batch idx 84: | train loss: 0.8525713086128235 | train accu: 0.496 | train roc: 1.000 | train f1: 0.42847444462982054\n",
      "batch idx 85: | train loss: 0.8256173133850098 | train accu: 0.539 | train roc: 1.000 | train f1: 0.467501797822067\n",
      "batch idx 86: | train loss: 0.8328235149383545 | train accu: 0.535 | train roc: 1.000 | train f1: 0.4690216266556291\n",
      "batch idx 87: | train loss: 0.7585292458534241 | train accu: 0.602 | train roc: 1.000 | train f1: 0.5391117454764285\n",
      "batch idx 88: | train loss: 0.7861605882644653 | train accu: 0.551 | train roc: 1.000 | train f1: 0.4857398005032351\n",
      "batch idx 89: | train loss: 0.8234200477600098 | train accu: 0.512 | train roc: 1.000 | train f1: 0.45289710461999366\n",
      "batch idx 90: | train loss: 0.8065813183784485 | train accu: 0.551 | train roc: 1.000 | train f1: 0.49064187199371756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx 91: | train loss: 0.7918955087661743 | train accu: 0.609 | train roc: 1.000 | train f1: 0.541664297088262\n",
      "batch idx 92: | train loss: 0.8643584251403809 | train accu: 0.500 | train roc: 1.000 | train f1: 0.43981804586241274\n",
      "batch idx 93: | train loss: 0.7458471655845642 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5493286400481686\n",
      "batch idx 94: | train loss: 0.8009623289108276 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5378639914772728\n",
      "batch idx 95: | train loss: 0.7563493251800537 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5453630277207782\n",
      "batch idx 96: | train loss: 0.7506570219993591 | train accu: 0.633 | train roc: 1.000 | train f1: 0.5763257575757575\n",
      "batch idx 97: | train loss: 0.7937060594558716 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5147903799363653\n",
      "batch idx 98: | train loss: 0.8254976272583008 | train accu: 0.516 | train roc: 1.000 | train f1: 0.4592775775105436\n",
      "batch idx 99: | train loss: 0.8196495175361633 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5214740094511088\n",
      "batch idx 100: | train loss: 0.7172989249229431 | train accu: 0.645 | train roc: 1.000 | train f1: 0.5998871056254186\n",
      "batch idx 101: | train loss: 0.824762761592865 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5003216435388919\n",
      "batch idx 102: | train loss: 0.8242303729057312 | train accu: 0.531 | train roc: 1.000 | train f1: 0.46173432881645227\n",
      "batch idx 103: | train loss: 0.8173497915267944 | train accu: 0.531 | train roc: 1.000 | train f1: 0.4635551369514306\n",
      "batch idx 104: | train loss: 0.7329100370407104 | train accu: 0.605 | train roc: 1.000 | train f1: 0.541268167390578\n",
      "batch idx 105: | train loss: 0.7873244881629944 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5064695789325164\n",
      "batch idx 106: | train loss: 0.8008154034614563 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5041790345258553\n",
      "batch idx 107: | train loss: 0.7263043522834778 | train accu: 0.641 | train roc: 1.000 | train f1: 0.5820821005917161\n",
      "batch idx 108: | train loss: 0.7205363512039185 | train accu: 0.641 | train roc: 1.000 | train f1: 0.5944956112540309\n",
      "batch idx 109: | train loss: 0.8082916140556335 | train accu: 0.547 | train roc: 1.000 | train f1: 0.5056242713494608\n",
      "batch idx 110: | train loss: 0.7826453447341919 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5121962338759214\n",
      "batch idx 111: | train loss: 0.7816324234008789 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5436154292861609\n",
      "batch idx 112: | train loss: 0.756400465965271 | train accu: 0.633 | train roc: 1.000 | train f1: 0.5811769715307908\n",
      "batch idx 113: | train loss: 0.8190963268280029 | train accu: 0.551 | train roc: 1.000 | train f1: 0.5029615752551021\n",
      "batch idx 114: | train loss: 0.781159520149231 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5156120390619108\n",
      "batch idx 115: | train loss: 0.7357016205787659 | train accu: 0.609 | train roc: 1.000 | train f1: 0.5562182433203928\n",
      "batch idx 116: | train loss: 0.7700095772743225 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5126678876678876\n",
      "batch idx 117: | train loss: 0.7918155193328857 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5320165492507893\n",
      "batch idx 118: | train loss: 0.7863091826438904 | train accu: 0.559 | train roc: 1.000 | train f1: 0.49064026591495635\n",
      "batch idx 119: | train loss: 0.730923056602478 | train accu: 0.617 | train roc: 1.000 | train f1: 0.5679380142330244\n",
      "Epoch: 06 | Epoch Time: 2m 10s\n",
      "\tTrain Loss: 0.792 | Train Acc: 56.95 | Train rocauc: 1.0 | Train f1: 0.5118360216254885%\n",
      "\t Val. Loss: 0.760 |  Val. Acc: 59.55 | Val. rocauc: 1.0 | Val. f1: 0.5367824343961931%\n",
      "batch idx 0: | train loss: 0.7752499580383301 | train accu: 0.551 | train roc: 1.000 | train f1: 0.5050683243727598\n",
      "batch idx 1: | train loss: 0.827146053314209 | train accu: 0.551 | train roc: 1.000 | train f1: 0.4936618293246993\n",
      "batch idx 2: | train loss: 0.7714781165122986 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5227727647035479\n",
      "batch idx 3: | train loss: 0.7547389268875122 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5289939150165017\n",
      "batch idx 4: | train loss: 0.8264170289039612 | train accu: 0.543 | train roc: 1.000 | train f1: 0.48258587786259544\n",
      "batch idx 5: | train loss: 0.7416495084762573 | train accu: 0.617 | train roc: 1.000 | train f1: 0.5789582198580314\n",
      "batch idx 6: | train loss: 0.764940083026886 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5280673785733839\n",
      "batch idx 7: | train loss: 0.8160524964332581 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5039537175890094\n",
      "batch idx 8: | train loss: 0.7390788793563843 | train accu: 0.609 | train roc: 1.000 | train f1: 0.5662526377438795\n",
      "batch idx 9: | train loss: 0.8209421038627625 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5000888170924935\n",
      "batch idx 10: | train loss: 0.7558043003082275 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5395449112021857\n",
      "batch idx 11: | train loss: 0.7920482158660889 | train accu: 0.551 | train roc: 1.000 | train f1: 0.49826888424304244\n",
      "batch idx 12: | train loss: 0.8184865117073059 | train accu: 0.516 | train roc: 1.000 | train f1: 0.4396276595744681\n",
      "batch idx 13: | train loss: 0.7614473104476929 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5260183744895975\n",
      "batch idx 14: | train loss: 0.7593005895614624 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5012956072181243\n",
      "batch idx 15: | train loss: 0.7697256207466125 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5228266939687267\n",
      "batch idx 16: | train loss: 0.7732686996459961 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5266153619197482\n",
      "batch idx 17: | train loss: 0.7623621225357056 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5235419732968881\n",
      "batch idx 18: | train loss: 0.7582974433898926 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5335722040987871\n",
      "batch idx 19: | train loss: 0.8014095425605774 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5088156470112317\n",
      "batch idx 20: | train loss: 0.7543538808822632 | train accu: 0.609 | train roc: 1.000 | train f1: 0.5511250072945845\n",
      "batch idx 21: | train loss: 0.78929203748703 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5084648553582566\n",
      "batch idx 22: | train loss: 0.7497788667678833 | train accu: 0.613 | train roc: 1.000 | train f1: 0.5684759297740836\n",
      "batch idx 23: | train loss: 0.7724035382270813 | train accu: 0.582 | train roc: 1.000 | train f1: 0.529940112750974\n",
      "batch idx 24: | train loss: 0.8595329523086548 | train accu: 0.520 | train roc: 1.000 | train f1: 0.4557867595886174\n",
      "batch idx 25: | train loss: 0.7558297514915466 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5548275699796262\n",
      "batch idx 26: | train loss: 0.8466726541519165 | train accu: 0.508 | train roc: 1.000 | train f1: 0.4512243635077794\n",
      "batch idx 27: | train loss: 0.7902461886405945 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5118020304568528\n",
      "batch idx 28: | train loss: 0.748781681060791 | train accu: 0.629 | train roc: 1.000 | train f1: 0.5847009892086331\n",
      "batch idx 29: | train loss: 0.8270164132118225 | train accu: 0.527 | train roc: 1.000 | train f1: 0.46671130952380957\n",
      "batch idx 30: | train loss: 0.8614494204521179 | train accu: 0.504 | train roc: 1.000 | train f1: 0.4425189393939394\n",
      "batch idx 31: | train loss: 0.7636701464653015 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5183851792123387\n",
      "batch idx 32: | train loss: 0.7603344917297363 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5325147367171736\n",
      "batch idx 33: | train loss: 0.7557899355888367 | train accu: 0.602 | train roc: 1.000 | train f1: 0.550657460387324\n",
      "batch idx 34: | train loss: 0.7916594743728638 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5147541749750748\n",
      "batch idx 35: | train loss: 0.784946858882904 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5169493835677408\n",
      "batch idx 36: | train loss: 0.8724619746208191 | train accu: 0.504 | train roc: 1.000 | train f1: 0.456948874328887\n",
      "batch idx 37: | train loss: 0.7689102292060852 | train accu: 0.609 | train roc: 1.000 | train f1: 0.5363879323953666\n",
      "batch idx 38: | train loss: 0.8158038854598999 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4936424509697985\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx 39: | train loss: 0.8606077432632446 | train accu: 0.516 | train roc: 1.000 | train f1: 0.4560136683481196\n",
      "batch idx 40: | train loss: 0.7914617657661438 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5313134854810452\n",
      "batch idx 41: | train loss: 0.761401355266571 | train accu: 0.605 | train roc: 1.000 | train f1: 0.5617382177681474\n",
      "batch idx 42: | train loss: 0.8235064744949341 | train accu: 0.559 | train roc: 1.000 | train f1: 0.4967499314692983\n",
      "batch idx 43: | train loss: 0.7522951364517212 | train accu: 0.613 | train roc: 1.000 | train f1: 0.5657204581993569\n",
      "batch idx 44: | train loss: 0.8083391785621643 | train accu: 0.559 | train roc: 1.000 | train f1: 0.4994736920688007\n",
      "batch idx 45: | train loss: 0.7610985636711121 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5056051826511544\n",
      "batch idx 46: | train loss: 0.7390798926353455 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5182542067307693\n",
      "batch idx 47: | train loss: 0.7946428656578064 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5164231601731601\n",
      "batch idx 48: | train loss: 0.829699695110321 | train accu: 0.559 | train roc: 1.000 | train f1: 0.4944237794012829\n",
      "batch idx 49: | train loss: 0.7825600504875183 | train accu: 0.543 | train roc: 1.000 | train f1: 0.48681853359321253\n",
      "batch idx 50: | train loss: 0.7686907649040222 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5495899956034802\n",
      "batch idx 51: | train loss: 0.7626813054084778 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5370062410415672\n",
      "batch idx 52: | train loss: 0.7739357948303223 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5203598186119874\n",
      "batch idx 53: | train loss: 0.721508800983429 | train accu: 0.629 | train roc: 1.000 | train f1: 0.5748361137218045\n",
      "batch idx 54: | train loss: 0.8091549873352051 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4925903502235469\n",
      "batch idx 55: | train loss: 0.8344326019287109 | train accu: 0.551 | train roc: 1.000 | train f1: 0.49281190877237013\n",
      "batch idx 56: | train loss: 0.7973154187202454 | train accu: 0.551 | train roc: 1.000 | train f1: 0.5049986471861472\n",
      "batch idx 57: | train loss: 0.747224748134613 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5279572740112994\n",
      "batch idx 58: | train loss: 0.7280421257019043 | train accu: 0.629 | train roc: 1.000 | train f1: 0.5796759628293339\n",
      "batch idx 59: | train loss: 0.7438710331916809 | train accu: 0.605 | train roc: 1.000 | train f1: 0.5536004149466804\n",
      "batch idx 60: | train loss: 0.7915757894515991 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5117330936352509\n",
      "batch idx 61: | train loss: 0.7774983644485474 | train accu: 0.617 | train roc: 1.000 | train f1: 0.5583568507746357\n",
      "batch idx 62: | train loss: 0.7600857615470886 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5487896425256996\n",
      "batch idx 63: | train loss: 0.8160300254821777 | train accu: 0.543 | train roc: 1.000 | train f1: 0.48464079310846175\n",
      "batch idx 64: | train loss: 0.7793105244636536 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5180816814994607\n",
      "batch idx 65: | train loss: 0.7540392279624939 | train accu: 0.598 | train roc: 1.000 | train f1: 0.528802243271315\n",
      "batch idx 66: | train loss: 0.7347072958946228 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5377289610257454\n",
      "batch idx 67: | train loss: 0.7796652913093567 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5095179736533725\n",
      "batch idx 68: | train loss: 0.807055652141571 | train accu: 0.539 | train roc: 1.000 | train f1: 0.4682742914979757\n",
      "batch idx 69: | train loss: 0.816071093082428 | train accu: 0.543 | train roc: 1.000 | train f1: 0.46453637584841634\n",
      "batch idx 70: | train loss: 0.8498441576957703 | train accu: 0.500 | train roc: 1.000 | train f1: 0.4416982604817128\n",
      "batch idx 71: | train loss: 0.8205296397209167 | train accu: 0.543 | train roc: 1.000 | train f1: 0.48404925519471576\n",
      "batch idx 72: | train loss: 0.8083556890487671 | train accu: 0.574 | train roc: 1.000 | train f1: 0.518294776869472\n",
      "batch idx 73: | train loss: 0.8307886719703674 | train accu: 0.516 | train roc: 1.000 | train f1: 0.4662311640372168\n",
      "batch idx 74: | train loss: 0.7918567061424255 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5353800402086708\n",
      "batch idx 75: | train loss: 0.7951034903526306 | train accu: 0.566 | train roc: 1.000 | train f1: 0.5239876279552658\n",
      "batch idx 76: | train loss: 0.7675806283950806 | train accu: 0.617 | train roc: 1.000 | train f1: 0.5704261739417988\n",
      "batch idx 77: | train loss: 0.7914189100265503 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5083680361142217\n",
      "batch idx 78: | train loss: 0.7524374127388 | train accu: 0.602 | train roc: 1.000 | train f1: 0.5473492616899098\n",
      "batch idx 79: | train loss: 0.8008983731269836 | train accu: 0.543 | train roc: 1.000 | train f1: 0.4723692092651757\n",
      "batch idx 80: | train loss: 0.7816005945205688 | train accu: 0.535 | train roc: 1.000 | train f1: 0.46560595504740077\n",
      "batch idx 81: | train loss: 0.8230031132698059 | train accu: 0.539 | train roc: 1.000 | train f1: 0.47372924428153673\n",
      "batch idx 82: | train loss: 0.7806804180145264 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4858881256634279\n",
      "batch idx 83: | train loss: 0.78099524974823 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5175806781045751\n",
      "batch idx 84: | train loss: 0.7579097151756287 | train accu: 0.617 | train roc: 1.000 | train f1: 0.5515827541361078\n",
      "batch idx 85: | train loss: 0.7768996357917786 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5280603853383459\n",
      "batch idx 86: | train loss: 0.8097921013832092 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4892484127307136\n",
      "batch idx 87: | train loss: 0.7978419661521912 | train accu: 0.551 | train roc: 1.000 | train f1: 0.48424416111783697\n",
      "batch idx 88: | train loss: 0.7372872829437256 | train accu: 0.605 | train roc: 1.000 | train f1: 0.5545998210139319\n",
      "batch idx 89: | train loss: 0.7934831976890564 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5267116078588613\n",
      "batch idx 90: | train loss: 0.7527692914009094 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5266413528726709\n",
      "batch idx 91: | train loss: 0.8098961114883423 | train accu: 0.551 | train roc: 1.000 | train f1: 0.49795487998188404\n",
      "batch idx 92: | train loss: 0.7250936031341553 | train accu: 0.621 | train roc: 1.000 | train f1: 0.5657891765857925\n",
      "batch idx 93: | train loss: 0.8394739031791687 | train accu: 0.543 | train roc: 1.000 | train f1: 0.4870524134597246\n",
      "batch idx 94: | train loss: 0.7708823680877686 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5429848622095663\n",
      "batch idx 95: | train loss: 0.808880627155304 | train accu: 0.555 | train roc: 1.000 | train f1: 0.5053796902508803\n",
      "batch idx 96: | train loss: 0.7405336499214172 | train accu: 0.605 | train roc: 1.000 | train f1: 0.5584962091829195\n",
      "batch idx 97: | train loss: 0.7253022193908691 | train accu: 0.652 | train roc: 1.000 | train f1: 0.5902723039645006\n",
      "batch idx 98: | train loss: 0.8099735975265503 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5205613169789999\n",
      "batch idx 99: | train loss: 0.7721595764160156 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5209604760802915\n",
      "batch idx 100: | train loss: 0.8215269446372986 | train accu: 0.531 | train roc: 1.000 | train f1: 0.46297191411417526\n",
      "batch idx 101: | train loss: 0.8312703371047974 | train accu: 0.543 | train roc: 1.000 | train f1: 0.48835152116402114\n",
      "batch idx 102: | train loss: 0.7968267798423767 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5042845158197508\n",
      "batch idx 103: | train loss: 0.7557072639465332 | train accu: 0.617 | train roc: 1.000 | train f1: 0.5742481448578596\n",
      "batch idx 104: | train loss: 0.7894862294197083 | train accu: 0.555 | train roc: 1.000 | train f1: 0.5099056368774797\n",
      "batch idx 105: | train loss: 0.8055443167686462 | train accu: 0.551 | train roc: 1.000 | train f1: 0.48699456682436426\n",
      "batch idx 106: | train loss: 0.7741044163703918 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5273567275747507\n",
      "batch idx 107: | train loss: 0.8281951546669006 | train accu: 0.535 | train roc: 1.000 | train f1: 0.47763665124049465\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx 108: | train loss: 0.7738227844238281 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5396487813600352\n",
      "batch idx 109: | train loss: 0.8255994319915771 | train accu: 0.527 | train roc: 1.000 | train f1: 0.46311648170756115\n",
      "batch idx 110: | train loss: 0.7635941505432129 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5048654878618113\n",
      "batch idx 111: | train loss: 0.8023726344108582 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4887126189983444\n",
      "batch idx 112: | train loss: 0.7709954977035522 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5247081644568723\n",
      "batch idx 113: | train loss: 0.7796867489814758 | train accu: 0.582 | train roc: 1.000 | train f1: 0.4981635334996436\n",
      "batch idx 114: | train loss: 0.7584115266799927 | train accu: 0.602 | train roc: 1.000 | train f1: 0.5421184289127838\n",
      "batch idx 115: | train loss: 0.7598327398300171 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5075116131756756\n",
      "batch idx 116: | train loss: 0.816075325012207 | train accu: 0.520 | train roc: 1.000 | train f1: 0.440990843222273\n",
      "batch idx 117: | train loss: 0.8221865296363831 | train accu: 0.547 | train roc: 1.000 | train f1: 0.47781702898550726\n",
      "batch idx 118: | train loss: 0.7675588130950928 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5406784180495973\n",
      "batch idx 119: | train loss: 0.7705420255661011 | train accu: 0.637 | train roc: 1.000 | train f1: 0.5769767923241769\n",
      "Epoch: 07 | Epoch Time: 2m 10s\n",
      "\tTrain Loss: 0.786 | Train Acc: 57.22 | Train rocauc: 1.0 | Train f1: 0.5147610609322398%\n",
      "\t Val. Loss: 0.757 |  Val. Acc: 60.41 | Val. rocauc: 1.0 | Val. f1: 0.5483427521303427%\n",
      "batch idx 0: | train loss: 0.7045179605484009 | train accu: 0.641 | train roc: 1.000 | train f1: 0.5912104510653999\n",
      "batch idx 1: | train loss: 0.8209916949272156 | train accu: 0.531 | train roc: 1.000 | train f1: 0.4770320921535698\n",
      "batch idx 2: | train loss: 0.7953212261199951 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5075500346786759\n",
      "batch idx 3: | train loss: 0.7866173386573792 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5225004475474401\n",
      "batch idx 4: | train loss: 0.7240968942642212 | train accu: 0.613 | train roc: 1.000 | train f1: 0.56190328854314\n",
      "batch idx 5: | train loss: 0.742743730545044 | train accu: 0.613 | train roc: 1.000 | train f1: 0.552220394736842\n",
      "batch idx 6: | train loss: 0.781385064125061 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5042770895454729\n",
      "batch idx 7: | train loss: 0.7566858530044556 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5345922837761448\n",
      "batch idx 8: | train loss: 0.8196506500244141 | train accu: 0.562 | train roc: 1.000 | train f1: 0.49693182658190427\n",
      "batch idx 9: | train loss: 0.7486751079559326 | train accu: 0.609 | train roc: 1.000 | train f1: 0.5697815533980584\n",
      "batch idx 10: | train loss: 0.7699577212333679 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5486424947169147\n",
      "batch idx 11: | train loss: 0.7867933511734009 | train accu: 0.551 | train roc: 1.000 | train f1: 0.5029515040920151\n",
      "batch idx 12: | train loss: 0.7493248581886292 | train accu: 0.609 | train roc: 1.000 | train f1: 0.5771963330786861\n",
      "batch idx 13: | train loss: 0.7923328280448914 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5204808404618135\n",
      "batch idx 14: | train loss: 0.7696571350097656 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5336536122386566\n",
      "batch idx 15: | train loss: 0.7991558909416199 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5114414785785019\n",
      "batch idx 16: | train loss: 0.8197057843208313 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4834479637663449\n",
      "batch idx 17: | train loss: 0.7688838839530945 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5538326217583809\n",
      "batch idx 18: | train loss: 0.7660951018333435 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5515418833615113\n",
      "batch idx 19: | train loss: 0.8270556926727295 | train accu: 0.551 | train roc: 1.000 | train f1: 0.49857196063250747\n",
      "batch idx 20: | train loss: 0.7777091264724731 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5141713285866512\n",
      "batch idx 21: | train loss: 0.7862583994865417 | train accu: 0.535 | train roc: 1.000 | train f1: 0.49646664688911635\n",
      "batch idx 22: | train loss: 0.787575900554657 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5388198313144825\n",
      "batch idx 23: | train loss: 0.7699303030967712 | train accu: 0.602 | train roc: 1.000 | train f1: 0.5489227723130334\n",
      "batch idx 24: | train loss: 0.7458257675170898 | train accu: 0.641 | train roc: 1.000 | train f1: 0.5919704861111111\n",
      "batch idx 25: | train loss: 0.7884880304336548 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5255350763105808\n",
      "batch idx 26: | train loss: 0.779064953327179 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5309911214688732\n",
      "batch idx 27: | train loss: 0.7782235741615295 | train accu: 0.613 | train roc: 1.000 | train f1: 0.5626990812343496\n",
      "batch idx 28: | train loss: 0.7973480820655823 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4780873081140351\n",
      "batch idx 29: | train loss: 0.8373385667800903 | train accu: 0.547 | train roc: 1.000 | train f1: 0.48573509954637095\n",
      "batch idx 30: | train loss: 0.7772659063339233 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5186056613788408\n",
      "batch idx 31: | train loss: 0.7620063424110413 | train accu: 0.566 | train roc: 1.000 | train f1: 0.49500510376086304\n",
      "batch idx 32: | train loss: 0.8105219602584839 | train accu: 0.539 | train roc: 1.000 | train f1: 0.46663844097565166\n",
      "batch idx 33: | train loss: 0.8180943131446838 | train accu: 0.539 | train roc: 1.000 | train f1: 0.47563682059322365\n",
      "batch idx 34: | train loss: 0.7620906233787537 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5175761036143619\n",
      "batch idx 35: | train loss: 0.7924808859825134 | train accu: 0.562 | train roc: 1.000 | train f1: 0.4924742199894236\n",
      "batch idx 36: | train loss: 0.7185918092727661 | train accu: 0.664 | train roc: 1.000 | train f1: 0.6103657302461141\n",
      "batch idx 37: | train loss: 0.813145637512207 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5242571207818243\n",
      "batch idx 38: | train loss: 0.8173069357872009 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5136562880842868\n",
      "batch idx 39: | train loss: 0.7654553651809692 | train accu: 0.582 | train roc: 1.000 | train f1: 0.5423985349348073\n",
      "batch idx 40: | train loss: 0.8357688188552856 | train accu: 0.539 | train roc: 1.000 | train f1: 0.46131413000526866\n",
      "batch idx 41: | train loss: 0.7840035557746887 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5142564472267344\n",
      "batch idx 42: | train loss: 0.7862939834594727 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5028068973732802\n",
      "batch idx 43: | train loss: 0.7759544253349304 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5235498536789298\n",
      "batch idx 44: | train loss: 0.7800577878952026 | train accu: 0.590 | train roc: 1.000 | train f1: 0.5227350524874488\n",
      "batch idx 45: | train loss: 0.8283128142356873 | train accu: 0.535 | train roc: 1.000 | train f1: 0.47399698289329795\n",
      "batch idx 46: | train loss: 0.8126348257064819 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5404454995296513\n",
      "batch idx 47: | train loss: 0.7754352688789368 | train accu: 0.609 | train roc: 1.000 | train f1: 0.5401874388827772\n",
      "batch idx 48: | train loss: 0.7679474949836731 | train accu: 0.594 | train roc: 1.000 | train f1: 0.5372258771929823\n",
      "batch idx 49: | train loss: 0.8001012206077576 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5052963863060016\n",
      "batch idx 50: | train loss: 0.7466976642608643 | train accu: 0.613 | train roc: 1.000 | train f1: 0.5530298454096375\n",
      "batch idx 51: | train loss: 0.8225457668304443 | train accu: 0.562 | train roc: 1.000 | train f1: 0.49718509152215795\n",
      "batch idx 52: | train loss: 0.781119704246521 | train accu: 0.586 | train roc: 1.000 | train f1: 0.532550702545069\n",
      "batch idx 53: | train loss: 0.774206280708313 | train accu: 0.586 | train roc: 1.000 | train f1: 0.5176395643625827\n",
      "batch idx 54: | train loss: 0.805431604385376 | train accu: 0.555 | train roc: 1.000 | train f1: 0.48902858259749826\n",
      "batch idx 55: | train loss: 0.7649573683738708 | train accu: 0.602 | train roc: 1.000 | train f1: 0.5298576797454042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch idx 56: | train loss: 0.7630661725997925 | train accu: 0.570 | train roc: 1.000 | train f1: 0.5074551458990536\n",
      "batch idx 57: | train loss: 0.7896111011505127 | train accu: 0.582 | train roc: 1.000 | train f1: 0.515315031949549\n",
      "batch idx 58: | train loss: 0.8120903968811035 | train accu: 0.555 | train roc: 1.000 | train f1: 0.4784647371828362\n",
      "batch idx 59: | train loss: 0.8123336434364319 | train accu: 0.551 | train roc: 1.000 | train f1: 0.48359837729742927\n",
      "batch idx 60: | train loss: 0.7809569835662842 | train accu: 0.582 | train roc: 1.000 | train f1: 0.539200141829715\n",
      "batch idx 61: | train loss: 0.7838171720504761 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5357277594136246\n",
      "batch idx 62: | train loss: 0.8138625025749207 | train accu: 0.547 | train roc: 1.000 | train f1: 0.4977378090659341\n",
      "batch idx 63: | train loss: 0.7826496362686157 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5369232110224103\n",
      "batch idx 64: | train loss: 0.7837482690811157 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5469957670707295\n",
      "batch idx 65: | train loss: 0.8079333901405334 | train accu: 0.559 | train roc: 1.000 | train f1: 0.5103543868186875\n",
      "batch idx 66: | train loss: 0.7793964743614197 | train accu: 0.570 | train roc: 1.000 | train f1: 0.512764811970133\n",
      "batch idx 67: | train loss: 0.7808167934417725 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5240144710578842\n",
      "batch idx 68: | train loss: 0.7561091184616089 | train accu: 0.609 | train roc: 1.000 | train f1: 0.553152340280796\n",
      "batch idx 69: | train loss: 0.7817577719688416 | train accu: 0.562 | train roc: 1.000 | train f1: 0.49595314648548083\n",
      "batch idx 70: | train loss: 0.7977835536003113 | train accu: 0.555 | train roc: 1.000 | train f1: 0.48810345006431266\n",
      "batch idx 71: | train loss: 0.759441077709198 | train accu: 0.598 | train roc: 1.000 | train f1: 0.5354985846852758\n",
      "batch idx 72: | train loss: 0.7846376895904541 | train accu: 0.562 | train roc: 1.000 | train f1: 0.49252903005464477\n",
      "batch idx 73: | train loss: 0.7272382378578186 | train accu: 0.617 | train roc: 1.000 | train f1: 0.5626056219017577\n",
      "batch idx 74: | train loss: 0.7774102687835693 | train accu: 0.574 | train roc: 1.000 | train f1: 0.5059586473108748\n",
      "batch idx 75: | train loss: 0.8079338073730469 | train accu: 0.543 | train roc: 1.000 | train f1: 0.47928242798719767\n",
      "batch idx 76: | train loss: 0.763740599155426 | train accu: 0.562 | train roc: 1.000 | train f1: 0.5069430734784284\n",
      "batch idx 77: | train loss: 0.7978562712669373 | train accu: 0.578 | train roc: 1.000 | train f1: 0.5151381625264004\n"
     ]
    }
   ],
   "source": [
    "history = {\n",
    "    \"train_loss\": [],\n",
    "    \"valid_loss\": []\n",
    "}\n",
    "\n",
    "import time\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(args['n_epochs']):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc, train_rocauc, train_f1 = train(model, train_loader, optimizer, criterion)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "    valid_loss, valid_acc, valid_rocauc, valid_f1 = evaluate(model, valid_loader, criterion)\n",
    "    history[\"valid_loss\"].append(valid_loss)\n",
    "    scheduler.step()\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "        \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} | Train rocauc: {train_rocauc} | Train f1: {train_f1}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f} | Val. rocauc: {valid_rocauc} | Val. f1: {valid_f1}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('best_model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "valid_loss, valid_acc, valid_rocauc, valid_f1 = evaluate(model, valid_loader, criterion)\n",
    "print(\"Valid loss: {} | Valid Acc: {:.3f} | Valid ROC-AUC: {} | Valid f1: {}\".format(\n",
    "    valid_loss, valid_acc, valid_rocauc, valid_f1))\n",
    "test_loss, test_acc, test_rocauc, test_f1 = evaluate(model, test_loader, criterion)\n",
    "print(\"Test loss: {} | Test Acc: {:.3f} | Test ROC-AUC: {} | Test f1: {}\".format(\n",
    "    test_loss, test_acc, test_rocauc, test_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(hist):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.plot(np.arange(1, len(history[\"train_loss\"]) + 1), history[\"train_loss\"], label=\"training loss\")\n",
    "    plt.plot(np.arange(1, len(history[\"train_loss\"]) + 1), history[\"valid_loss\"], label=\"validation loss\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.title(\"Training and Validation Losses\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
