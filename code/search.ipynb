{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x20587283890>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.metrics import f1_score, roc_auc_score\n",
    "import time\n",
    "\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_FAIL, STATUS_OK\n",
    "import csv\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EVALS = 1\n",
    "output_filename = \"./search_result/search.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the dataset and data iterators\n",
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, x, labels):\n",
    "        'Initialization'\n",
    "        self.x = x\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.x.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "\n",
    "        # Load data and get label\n",
    "        x = self.x[index]\n",
    "        y = self.labels[index]\n",
    "\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTGRUSentiment(nn.Module):\n",
    "    def __init__(self,\n",
    "                 bert,\n",
    "                 hidden_dim,\n",
    "                 output_dim,\n",
    "                 n_layers,\n",
    "                 bidirectional,\n",
    "                 dropout):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.bert = bert\n",
    "        \n",
    "        embedding_dim = bert.config.to_dict()['dim']\n",
    "        \n",
    "        self.rnn = nn.GRU(embedding_dim,\n",
    "                          hidden_dim,\n",
    "                          num_layers = n_layers,\n",
    "                          bidirectional = bidirectional,\n",
    "                          batch_first = True,\n",
    "                          dropout = 0 if n_layers < 2 else dropout)\n",
    "        \n",
    "        self.out = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        \n",
    "        #text = [batch size, sent len]\n",
    "        attention_mask = text.masked_fill(text != 0, 1)\n",
    "                \n",
    "        with torch.no_grad():\n",
    "            embedded = self.bert(text, attention_mask=attention_mask)[0]\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        _, hidden = self.rnn(embedded)\n",
    "        \n",
    "        #hidden = [n layers * n directions, batch size, emb dim]\n",
    "        \n",
    "        if self.rnn.bidirectional:\n",
    "            hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        else:\n",
    "            hidden = self.dropout(hidden[-1,:,:])\n",
    "                \n",
    "        #hidden = [batch size, hid dim]\n",
    "        \n",
    "        output = self.out(hidden)\n",
    "        \n",
    "        #output = [batch size, out dim]\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_len(tokenized):\n",
    "    max_len = 0\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "    return max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_acc(y_pred, y_label):\n",
    "    softmax = nn.Softmax(dim=1)\n",
    "    y_pred_softmax = softmax(y_pred)\n",
    "    _, y_pred_tags = torch.max(y_pred_softmax, dim = 1)\n",
    "\n",
    "    # accu\n",
    "    correct_pred = (y_pred_tags == y_label).float()\n",
    "    acc = correct_pred.sum() / len(y_label)\n",
    "\n",
    "    # roc-auc\n",
    "    one_hot_label = nn.functional.one_hot(y_label)\n",
    "    roc_auc = roc_auc_score(one_hot_label.detach().cpu(), y_pred_softmax.detach().cpu(), average=\"macro\")\n",
    "\n",
    "    # f1\n",
    "    f1 = f1_score(y_label.detach().cpu(), y_pred_tags.detach().cpu(), average='weighted')\n",
    "    \n",
    "    return acc, roc_auc, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_rocauc = 0\n",
    "    epoch_f1 = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(data_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(data).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, target)\n",
    "        \n",
    "        acc, roc_auc, f1 = multi_acc(predictions, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        epoch_rocauc += roc_auc\n",
    "        epoch_f1 += f1\n",
    "\n",
    "#         print(\"batch idx {}: | train loss: {} | train accu: {:.3f} | train roc: {:.3f} | train f1: {}\".format(\n",
    "#             batch_idx, loss.item(), acc.item(), roc_auc, f1))\n",
    "        \n",
    "    return epoch_loss / len(data_loader), epoch_acc / len(data_loader), epoch_rocauc / len(data_loader), epoch_f1 / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    epoch_rocauc = 0\n",
    "    epoch_f1 = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch_idx, (data, target) in enumerate(data_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            predictions = model(data).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, target)\n",
    "            \n",
    "            acc, roc_auc, f1 = multi_acc(predictions, target)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "            epoch_rocauc += roc_auc\n",
    "            epoch_f1 += f1\n",
    "        \n",
    "    return epoch_loss / len(data_loader), epoch_acc / len(data_loader), epoch_rocauc / len(data_loader), epoch_f1 / len(data_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(space):\n",
    "    args = {'batch_size': 128,\n",
    "    'lr': space['lr'],\n",
    "    'hidden_dim': 128,\n",
    "    'n_layers': space['n_layers'],\n",
    "    'bidirectional': True,\n",
    "    'dropout': space['dropout'],\n",
    "    'n_epochs': 20,\n",
    "    'b1': space['b1'],\n",
    "    'b2': space['b2'],\n",
    "    'weight_decay': space['weight_decay'],\n",
    "    'weight': torch.tensor([0.1568, 0.4639, 0.3793], dtype=torch.float32)\n",
    "    }\n",
    "    train_loader = torch.load(\"train_loader.pth\")\n",
    "    valid_loader = torch.load(\"valid_loader.pth\")\n",
    "    test_loader = torch.load(\"test_loader.pth\")\n",
    "    \n",
    "    opt_name = '_'.join(['b1_'+str(args['b1']), 'b2_'+str(args['b2']), 'lr'+str(args['lr']),\n",
    "                         'drop'+str(args['dropout']), 'l2_'+str(args['weight_decay'])])\n",
    "    \n",
    "    bert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "    model = BERTGRUSentiment(bert,\n",
    "                         args['hidden_dim'],\n",
    "                         3,\n",
    "                         args['n_layers'],\n",
    "                         args['bidirectional'],\n",
    "                         args['dropout']).to(device)\n",
    "    for name, param in model.named_parameters():                \n",
    "        if name.startswith('bert'):\n",
    "            param.requires_grad = False\n",
    "    optimizer = optim.Adam(model.parameters(), \n",
    "                           lr=args['lr'], \n",
    "                           betas=(args[\"b1\"], args[\"b2\"]),\n",
    "                           weight_decay=args[\"weight_decay\"])\n",
    "    criterion = nn.CrossEntropyLoss(weight=args['weight']).to(device)\n",
    "    history = {\n",
    "        \"train_loss\": [],\n",
    "        \"valid_loss\": []\n",
    "    }\n",
    "\n",
    "    best_valid_loss = float('inf')\n",
    "    best_valid_acc = 0\n",
    "    best_valid_f1 = 0\n",
    "\n",
    "    for epoch in range(args['n_epochs']):\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        train_loss, train_acc, train_rocauc, train_f1 = train(model, train_loader, optimizer, criterion)\n",
    "        history[\"train_loss\"].append(train_loss)\n",
    "        valid_loss, valid_acc, valid_rocauc, valid_f1 = evaluate(model, valid_loader, criterion)\n",
    "        history[\"valid_loss\"].append(valid_loss)\n",
    "\n",
    "        end_time = time.time()\n",
    "\n",
    "        epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            best_valid_acc = valid_acc\n",
    "            best_valid_f1 = valid_f1\n",
    "            \n",
    "#         print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "#         print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f} | Train rocauc: {train_rocauc} | Train f1: {train_f1}%')\n",
    "#         print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f} | Val. rocauc: {valid_rocauc} | Val. f1: {valid_f1}%')\n",
    "    model_dict = {\n",
    "        \"v_loss\": best_valid_loss,\n",
    "        \"v_acc\": best_valid_acc,\n",
    "        \"v_f1\": best_valid_f1,\n",
    "        \"name\": opt_name\n",
    "    }\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                            | 0/1 [00:00<?, ?trial/s, best loss=?]"
     ]
    }
   ],
   "source": [
    "# hyper parameter search space'\n",
    "space = {\n",
    "    'lr': hp.loguniform('lr', np.log(1e-4), np.log(3e-2)),\n",
    "    'n_layers': hp.choice(\"n_layers\", range(2, 4, 1)),\n",
    "    'dropout': hp.uniform(\"dropouut\", 0.25, 0.5),\n",
    "    'b1': hp.loguniform('b1', np.log(0.5), np.log(0.9)),\n",
    "    'b2': hp.loguniform('b2', np.log(0.5), np.log(0.999)),\n",
    "    'weight_decay': hp.loguniform('weight_decay', np.log(0.01), np.log(1))\n",
    "}\n",
    "\n",
    "trials = Trials()\n",
    "evals_inc = min(NUM_EVALS, 1)\n",
    "while evals_inc <= NUM_EVALS:\n",
    "    best = fmin(fn=run_model, space=space, algo=tpe.suggest, max_evals=evals_inc,\n",
    "                trials=trials)\n",
    "    results  = []\n",
    "    for trial in trails.trails:\n",
    "        results.append(trail['result'])\n",
    "    keys = results[0].keys()\n",
    "    with(open(output_filename, \"w\")) as output_file:\n",
    "        dict_writer = csv.DictWriter(output_filename, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(results)\n",
    "        \n",
    "    if evals_inc == NUM_EVALS:\n",
    "        break\n",
    "    evals_inc = min(NUM_EVALS, evals_inc+5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
